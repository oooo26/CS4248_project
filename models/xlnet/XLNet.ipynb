{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq2NOE3qH46J"
      },
      "source": [
        "# Introduction\n",
        "\n",
        "In this tutorial, I'll show you how to finetune the pretrained XLNet model with the huggingface PyTorch library to quickly produce a classifier for text classification.\n",
        "\n",
        "(This post follows the previous post on finetuning BERT very closely, but uses the updated interface of the huggingface library (pytorch-transformers) and customizes the input for use in XLNet.)\n",
        "\n",
        "This post is presented in two forms–as a blog post here and as a Colab notebook here. The content is identical in both, but:\n",
        "\n",
        "The blog post format may be easier to read, and includes a comments section for discussion.\n",
        "The Colab Notebook will allow you to run the code and inspect it as you read through.\n",
        "\n",
        "## What is XLNet?\n",
        "\n",
        "XLNet is a method of pretraining language representations developed by CMU and Google researchers in mid-2019. XLNet was created to address what the authors saw as the shortcomings of the autoencoding method of pretraining used by BERT and other popular language models. We won't get into the details of XLNet in this post, but the authors favored a custom autoregressive method. This pretraining method resulted in models that outperformed BERT on a range of NLP tasks and resulted in a new state of the art model. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RX_ZDhicpHkV"
      },
      "source": [
        "## Install and Import"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nSU7yERLP_66"
      },
      "source": [
        "Google Colab offers free GPUs and TPUs! Since we'll be training a large neural network it's best to take advantage of this (in this case we'll attach a GPU), otherwise training will take a very long time.\n",
        "\n",
        "A GPU can be added by going to the menu and selecting:\n",
        "\n",
        "Edit -> Notebook Settings -> Add accelerator (GPU)\n",
        "\n",
        "Then run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEfSbAA4QHas",
        "outputId": "c4efce64-cc31-46f8-8ab7-5a276d97830c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ElsnSNUridI"
      },
      "source": [
        "Next, let's install the pytorch interface for XLNet by Hugging Face. (This library contains interfaces for other pretrained language models like OpenAI's GPT, BERT, and GPT-2.) We've selected the pytorch interface because it strikes a nice balance between the high-level APIs (which are easy to use but don't provide insight into how things work) and tensorflow code (which contains lots of details but often sidetracks us into lessons about tensorflow)\n",
        "\n",
        "At the moment, the Hugging Face library seems to be the most widely accepted and powerful pytorch interface for working with transfer learning models. In addition to supporting a variety of different pre-trained language models (and future models to come - just a few short months after the publication of BERT and XLNet, both have been outperformed by new models!), the library also includes pre-built modifications of different models suited to your specific task. For example, in this tutorial we will use XLNetForSequenceClassification, but the library also includes model modifications designed for token classification, question answering, next sentence prediciton, etc. Using these pre-built classes simplifies the process of modifying transfer learning models for your purposes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0NmMdkZO8R6q",
        "outputId": "c932bbc3-93d7-4abe-8352-bca5451e4a01"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m71.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.27.1)\n",
            "Collecting huggingface-hub<1.0,>=0.11.0\n",
            "  Downloading huggingface_hub-0.13.3-py3-none-any.whl (199 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.8/199.8 KB\u001b[0m \u001b[31m24.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.10.7)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m100.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.13.3 tokenizers-0.13.2 transformers-4.27.4\n"
          ]
        }
      ],
      "source": [
        "!pip install pytorch-transformers\n",
        "!pip install Keras-Preprocessing\n",
        "!pip install transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ok002ceNB8E7"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "from keras_preprocessing.sequence import pad_sequences\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from transformers import XLNetModel, XLNetTokenizer, XLNetForSequenceClassification\n",
        "from transformers import AdamW\n",
        "\n",
        "from tqdm import tqdm, trange\n",
        "import pandas as pd\n",
        "import io\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# % matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqG7FzRVFEIv"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in our training loop, we will load data onto the device. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "oYsV4H8fCpZ-",
        "outputId": "cc0cf87c-c78c-477f-c328-76400245e248"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Tesla T4'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "n_gpu = torch.cuda.device_count()\n",
        "torch.cuda.get_device_name(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "guw6ZNtaswKc"
      },
      "source": [
        "## Load Dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "DOpikSXbsvn-",
        "outputId": "1a459bab-3a58-4a7f-e64d-e1ec6a69a0d5"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-a5cf84c6-6882-497b-8307-be50408d95cc\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-a5cf84c6-6882-497b-8307-be50408d95cc\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving fulltrain.csv to fulltrain.csv\n"
          ]
        }
      ],
      "source": [
        "# Upload the train file from your local drive\n",
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_UkeC7SG2krJ"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"fulltrain.csv\", names = ['label', 'text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3UfxtwQy3axu",
        "outputId": "1e7e8dc3-def4-40fe-ca0a-b76dbc01cc70"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(48854, 2)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "AQfTaYDo42zu",
        "outputId": "93541226-ba67-460d-d318-f3f1502bad95"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-76b42533-e493-4c5f-9b35-45a97c815ce4\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>label</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>11198</th>\n",
              "      <td>1</td>\n",
              "      <td>The CIA announced Monday that it suspects Sadd...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25572</th>\n",
              "      <td>3</td>\n",
              "      <td>The Self-Driving Car is Going to End in Disast...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2089</th>\n",
              "      <td>1</td>\n",
              "      <td>Tearing past crew members and camera equipment...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4070</th>\n",
              "      <td>1</td>\n",
              "      <td>Leading basketball experts predicted yesterday...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>44385</th>\n",
              "      <td>4</td>\n",
              "      <td>About 10 years ago, when I was working in Fran...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8925</th>\n",
              "      <td>1</td>\n",
              "      <td>At a press conference Monday, NFL officials to...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39351</th>\n",
              "      <td>4</td>\n",
              "      <td>Teenager Umar Akmal smashed an unbeaten 102 of...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14792</th>\n",
              "      <td>2</td>\n",
              "      <td>Hillary Gets The Worst News Of Her Life... Its...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16822</th>\n",
              "      <td>2</td>\n",
              "      <td>BREAKING: Gay Couples In Texas Just Lost Their...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32311</th>\n",
              "      <td>3</td>\n",
              "      <td>Interplanetary Communication With Robots Teste...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-76b42533-e493-4c5f-9b35-45a97c815ce4')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-76b42533-e493-4c5f-9b35-45a97c815ce4 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-76b42533-e493-4c5f-9b35-45a97c815ce4');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "       label                                               text\n",
              "11198      1  The CIA announced Monday that it suspects Sadd...\n",
              "25572      3  The Self-Driving Car is Going to End in Disast...\n",
              "2089       1  Tearing past crew members and camera equipment...\n",
              "4070       1  Leading basketball experts predicted yesterday...\n",
              "44385      4  About 10 years ago, when I was working in Fran...\n",
              "8925       1  At a press conference Monday, NFL officials to...\n",
              "39351      4  Teenager Umar Akmal smashed an unbeaten 102 of...\n",
              "14792      2  Hillary Gets The Worst News Of Her Life... Its...\n",
              "16822      2  BREAKING: Gay Couples In Texas Just Lost Their...\n",
              "32311      3  Interplanetary Communication With Robots Teste..."
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2sPDL2qOTxSU"
      },
      "outputs": [],
      "source": [
        "# Create sentence and label lists\n",
        "from sklearn.model_selection import train_test_split\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import nltk\n",
        "texts = df.text.values\n",
        "labels = df.label.values - 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTwbInroL7kR",
        "outputId": "caf9447a-6123-4373-ba04-67f5b6556e1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting unidecode\n",
            "  Downloading Unidecode-1.3.6-py3-none-any.whl (235 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m235.9/235.9 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: unidecode\n",
            "Successfully installed unidecode-1.3.6\n"
          ]
        }
      ],
      "source": [
        "!pip install unidecode\n",
        "import unidecode\n",
        "import re\n",
        "import string"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VD4eTl03TmUq"
      },
      "source": [
        "We need to add special tokens (\"[SEP]\" and \"[CLS]\") at the beginning and end of each sentence for XLNet to work properly. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GuE5BqICAne2",
        "outputId": "1a3f35a0-62d1-45cd-ad1f-0d83f52f5f6b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "['a little less than a decade ago hockey fans were blessed with a slate of games every night but on thursday sources confirmed that for the ninth consecutive year nhl players have been locked out with very slim hopes of an agreement in sight it seems like just yesterday martin st louis and his lightning teammates were raising the stanley cup high school hockey coach and onetime espn analyst barry melrose said obviously im still hoping the two sides can come together and reach an agreement but im starting to think nobody really misses hockey anymore nope nobody but old barry id still love to catch an atlanta thrashers game observers have noted that when arena doors do reopen the nhl will face the perhaps greater challenge of convincing fans to return to hockey instead of watching more popular sports like football basketball baseball and slamball [SEP] [CLS]',\n",
              " 'the writers of the hbo series the sopranos took another daring storytelling step by killing off 10 million fans during the seventh seasons premiere episode sunday night this was definitely a bold choice one that producers of the show would have never thought of making five years ago said new york times television critic virginia heffernan who noted that the move was hinted at in a seasonfive episode in which tony dreamt he was riding a horse through his house but now that i look back this was strongly foreshadowed throughout all of last season industry insiders predicted that the shows producers would try to bring at least some fans back for the series finale which may come as early as may [SEP] [CLS]',\n",
              " 'despite claims from the tv news outlet to offer nonstop news and coverage you can count on an selfnews investigation has uncovered hundreds of instances in which kamr channel 4 10 oclock eyewitness news team relied almost exclusively on news reports weather forecasts and even specialinterest features already generated by the stations 6 oclock eyewitness news team the investigation found that 10 oclock news team is in fact not the team you can trust in an examination of 98 consecutive primetime and latenight broadcasts including dozens more nationwide the amarillobased stationthe regions selfstyled news leaderrepeatedly ran pieces for its health beat pet patrol and bargain busters segments in both evening news slots and regularly relayed the same weather updates and traffic reports up to 15 times a day kamr even routinely rehashed 6 pm footage for seemingly urgent breaking news reports most recently the plum creek nursing home power outage and the bonham middle school roof collapse in an april incident involving the 10 pm recap of a local cancer fun run anchor andy justus read almost the exact same copy introducing the piece as he had just four hours earlier while reporter shalandys anderson altered only one word between broadcasts changing heartwarming to inspiring if theyre on our side as they claim what then is a purportedly professional news team doing in the four hours between broadcasts amarillo resident and frequent localnews viewer mark jette said during another 10 pm broadcast live continuing coverage from reporter matt orlando of a twoalarm elwood park house fire consisted almost wholly of previously aired footage of the firefighters in action the lack of updated footage disappointed viewers such as hereford txs kelly byer whose mild curiosity about the blaze first piqued at the 6 pm newscast went ungratified h3well continue to watch this important breaking news storyh3pelizabeth dinh after a report on a broken gas main which had already run in two previous newscastsp its true that the image of that scorched little doll was powerful and may have bore repeating but where was the followup footage of the devastated family at a red cross shelter byer said or some fresh broll of the charred ruins of the house the public deserves better the investigation also found the 10 pm kamr broadcast consistently reaired closing stock numbers and highschool baseball highlights and sports bloopers its producers and anchors apparently unaware or indifferent to the fact that the information was hours old and already common knowledge among viewers they say that the 6 oclock news team is the areas most watched news team jette said especially by the 10 oclock news team just last tuesday investigative reporter meaghan colliers problem solvers segment on squalid conditions at a local dog kennel aired again at 10 pm without even a cursory update on the brokenlegged puppy featured in the report their scorching summer coverage was even worse jette continued how many times do you have to repeat the same cool tips before all of amarillo is crystal clear on exactly how to beat the heat while kamr was a particularly flagrant offender it is by no means alone in a segment about the san diego zoos baby pandas kfmb tv8s news at 11 not only offered footage identical to the previous telecast but practically indistinguishable coos of affection from the coanchor at some stations the problem goes far beyond onetime reuse an 11 pm segment on heartsmart dinner alternatives on new haven connecticuts wtnh channel 8 was not only previously seen on the 6 pm news but also on live at 5 the 4 report and the news at noon with sonia baghdady another piece on the citys aging school buses was rotated into the following days good morning new haven as well in extreme examples such as in louisville kys wlkytv primetime and latenight newscasts the only distinguishable characteristic is the lead anchors concluding suggestion to stay tuned for david lettermans the late show despite the mounting controversy over the kamr channel 4 team the investigation was unable to conclusively prove that the hopeful wishes to see viewers the next day and the camaraderie and laughter shared between coanchors kyla cullinane and elizabeth dinh were anything less than genuine [SEP] [CLS]',\n",
              " 'after receiving subpar service and experiencing an unusually long wait for his 475 lunch at a local beefside family restaurant monday customer gus oconnor opted to give waitress carla hyams a reduced 10 percent tip in an attempt to communicate his dissatisfaction and raise awareness of the areas in which he felt her performance was lacking oconnor hoped his reduced tip would be a wakeup call for hyams hyams 49 who has been serving tables at the popular eatery for 13 years expressed enthusiastic gratitude for the immense personal growth the gesture will afford her adding that in the long run the experience will make her a better waitress maybe i was a little short with him when i told him to hold on a sec but in the future ill do my best to ensure a situation like that never ever happens again said hyams who put oconnors order slip in as the understaffed cooks dealt with a large complicated meal for a busload of seniorcitizen tourists its days like this that i thank god i get paid less than minimum wage and can rely on a builtin economic incentive to keep me motivated during those 16hour double shifts hyams added that she now knows she should always bring a glass of water without any ice cubes every time someone orders a diet coke and that the phrase when you get a minute is in fact a polite way of indicating that the customer wants his request filled in under one minute if he hadnt withheld that 50 cents id make these same mistakes over and over for the rest of my career she said even at my age its amazing to think you can still learn something new about a lowpaying meniallabor job hyams added that the next time she sees oconnor she will remember that he undertipped her and strive to serve him better to avoid any further disappointment oconnor he may not realize it but his actions today will not only improve my work ethic but will directly benefit him as well in that i will gain economic and personal rewards by treating him with the tremendous respect and unfailing attention he deserves hyams said so really if you think about it that 10 percent tip is a winwin situation for both of us oconnor said he felt he needed to get through to the waitress and did so the best way he knew how by giving her less than the universally agreedupon minimum i sent a clear unmistakable yet constructive message said oconnor who claimed that he hoped the smaller tip would be a wakeup call for hyams i was just trying to help push carla along the path to achieving her full potential as an employee it was the absolute least i could do he added oconnor said he first considered reducing his usual 15 percent tip for the waitress when hyams failed to replace the cream packets for his coffee while he looked over the restaurants extensive list of lunch specials but it wasnt until hyams neglected to ask if he needed extra ketchup that oconnor made the decision to let his money do the talking in the competitive service industry there is a mechanism to effect change he said i know this will be an invaluable lesson she wont soon forget but i just did what any decent human being in my position would have done and that feels good oconnor said his overall goal was not only to receive better service but to help hyams become a role model for her two teenage children tyler and michael i know as well as anyone how hard it is for a single mother with a limited income to raise kids on her own he said but this way they learn the value of money and the satisfaction of a job well done in the end hyams said she could not agree more my boys have had a few runins with the law and they could certainly use some good advice she said i cant wait for them and maybe a couple of their friends to meet mr oconnor firsthand i think theyd get a lot out of it [SEP] [CLS]',\n",
              " 'after watching his beloved seattle mariners prevail against the san diego padres thirdgrader timmy hastert was moved to ask his father 46yearold insurance salesman christopher hastert why interleague play is good well it lets people see the teams they normally dont get to see all that often i think is the point there buddy hastert said after beginning three different sentences in seven minutes after all without interleague play we wouldnt get to see players like like brian giles and scott linebrink would we although i think we play the yankees and red sox less often as a result right yeah im pretty sure thats how they do it in a moving but ultimately doomed effort to give his impressionable boy the right messages christopher also attempted to answer timmys questions regarding why only the american league has a dh why and how the allstar game now counts what performanceenhancing drugs are and how baseball officials could have sat idly by when they knew there was a major steroid problem in their sport [SEP] [CLS]',\n",
              " 'at a cafeteriatable press conference monday david pernell 10 categorically denied girlliking allegations recently levied against him by fellow lakeview elementary school fourthgrader jonathan witt i do not have not and will not ever like girls pernell told the crowd of seven boys assembled at the lunchrooms back table mr witts accusations are not only 100 percent false but also slanderous as it has always been my firm conviction that girls are totally and completely gross pernell went on to suggest that perhaps it is witt who likes girls particularly jenny loughlin 10 [SEP] [CLS]',\n",
              " 'stunned shock and dismay were just a few of the reactions from bobby guntergrass on tuesday when the 10yearold learned that the woman from the guinness book of world records who has the ability to pop her bulging eyeballs nearly all the way out of their sockets was not a millionaire no way shes gotta to be rich said guntergrass adding that if he himself possessed the womans unique attribute he would be a billionaire maybe she spent it all on a giant house yeah that makes sense she probably bought a huge mansion at press time guntergrass had reportedly grown even more disillusioned after learning that the man who holds the record for the worlds longest fingernails lives in a developing nation and has no access to clean water [SEP] [CLS]',\n",
              " 'speaking with reporters before a game monday local little leaguer nathan garrett expressed his heartfelt wish that just once his unemployed father could not make it out to see him play every time i come up to bat i hope deep down inside that this will be the time that i look up into the stands and dont see my dad said the visibly downcast 10yearold confirming that his father peter garrett a former hospital records clerk who has been out of work for 15 months had not missed even one game all season i look down the rows of bleachers but hes always there and when i see him my heart just sinks i would give anything for him to be in an office not watching me play garrett added the fourthgrader admitted he often feels envious of his teammates whose steadily employed fathers are either unable to make it to games or only show up for the final inning or two in addition garrett reported being especially saddened when his dad failed to miss even the most inconveniently timed games right after school in the middle of the week stating that he felt embarrassed and ashamed that everyone else on the team might notice that his father was the only parent in the stands with his father never putting in long hours at work garrett acknowledged that he has devised ways to help him cope with the constant attention he receives i try to put the whole thing out of my mind but sometimes its really hard not to think about how dad isnt stuck behind a desk somewhere especially when he stands up and cheers my name said garrett sullenly noting that the number of games left in the season that his father could potentially miss were rapidly running out and once i start thinking about how he came to yet another game and how hell probably come to the next one too i just feel awful i guess dad will always be the one whistling and clapping for me in the stands instead of furthering a career garrett continued and thats just something i have to live with in addition to showing up at games garretts longunemployed father is said to frequently disappoint his son with his constant availability at home as well in particular garrett stated that his dad who hasnt had a job interview in months is always ready to play catch in the backyard at any time drive him to and from practice at all hours and even help out with school or boy scouts projects moreover garrett noted with a dejected sigh that his father frequently offers to take him to the local minor league ballpark in spite of his frustrations with his fathers behavior garrett said that ever since he joined little league last springshortly after his dad was laid off from the job hed held for over a decadehe has clung to a sliver of hope that his father might one day apologize and offer an excuse about having important work to take care of instead of showing up i just want dad to come up to me before a game put his hand on my shoulder and finally say im not going to be there tonight champ said the second baseman for the leagues blue jays club if he only knew how much i think about walking off the field after a game without being hugged or congratulated thats all i want ultimately garrett said that when he later looks back at this time in his life he wants to remember his father having work obligations that prevented him from attending his sons extracurricular activities i just wish dad would realize how much it means to me that he not be here said garrett before taking the field when it comes down to it i just want my dad to have a little less time for me you know the 10yearold then reportedly covered the tears welling in his eyes with his glove after hearing the phrase go get em nate shouted from behind the dugout [SEP] [CLS]',\n",
              " 'sports journalists and television crews were pushed aside during super bowl media day on tuesday as more than 1000 writers for the website bleacherreportcom entered lucas oil stadium to acquire material for their trademark style of reportage i asked tom bardy sic where he thought he should be on my list of the top 10 guys to ever play in the super bowl and he said it didnt sound like i had anyone who played before 2005 said bleacher report writer darron nasty 16 whose credentials identified him as writer top 10 carolina and miami hurricanes writer june 2011 i think we got a lot of good answers though and our super bowl analysis lists are going to give a lot of insight to the fans and coaching staffs also at media day for the first time was the sports and culture web magazine grantland which sent vin scully to narrate the proceedings so that editors could sit at home listening to him on the radio while drinking cognac [SEP] [CLS]',\n",
              " 'salem vafor the eighth straight worldhistory period sophomores at riverside high school watched the 1959 classic benhur tuesday the chariot races were pretty cool michael bower said of the 211minute film he and classmates have been watching in 25minute segments between roll call and freereading and when mr franks got back from the teachers lounge he told us jesus is in tomorrows part bower said he dreads next week when the class will break into benhur discussion groups and share their ancienthistory unit journals [SEP] [CLS]']"
            ]
          },
          "execution_count": 57,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt')\n",
        "def preprocess_text(text_arr):\n",
        "    output_text = []\n",
        "    for x in text_arr:\n",
        "        processed = str(unidecode.unidecode(re.sub('[%s]' % re.escape(string.punctuation), '' , x))).lower()\n",
        "        sents = [s + \" [SEP] [CLS]\" for s in sent_tokenize(processed)]\n",
        "\n",
        "\n",
        "        output_text.append(' '.join(sents))\n",
        "        \n",
        "    return output_text\n",
        "    # sentence + \" [SEP] [CLS]\"\n",
        "sentences = preprocess_text(texts)\n",
        "sentences[:10]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ex5O1eV-Pfct"
      },
      "source": [
        "## Inputs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BTREubVNFiz4"
      },
      "source": [
        "Next, import the XLNet tokenizer, used to convert our text into tokens that correspond to XLNet's vocabulary."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119,
          "referenced_widgets": [
            "1b0dd29367694dc7939f1a906ba58424",
            "52d0e753d1094ac0bf4c510f23daf377",
            "de75cff83a814914b5611bdfd2d04139",
            "024d1c3eb53849638b4ac41b65095bb2",
            "5d64b41c019949b48f12e637597cadba",
            "1f3f8243ff4c4a73b954d7f74b1e9d9e",
            "d5d3619fede64862a0c0d5c0e3dee965",
            "ead0975fc9464153803e7037c40d4a5e",
            "3be949c1839b4f2a81db3c22721e9da7",
            "49db97d095bd45feb29d24ff992efd1b",
            "8a673928f54f4fb18fb4dc95d632a579",
            "6148540e8507424bb3be6608447e95cd",
            "26810a377f0b464b8e4213bc7f916b03",
            "4c73f89368a24638813e2a91ac5a28ae",
            "b45acac8abb2485fa9f84a84ff5d34c2",
            "c9855f7ddba8429dbff90de71079e2d9",
            "64eb473908524150a5d117703edecc2d",
            "d1413ac14e984be29c443813adabaa53",
            "f467ee5e7a4a410e9785777ebcbf578d",
            "feabf21450ec4707b7ffe4661377e928",
            "5248358598d14cd4a926681e7dad7204",
            "6a818b9950234ba5b5601d652287cb38"
          ]
        },
        "id": "Z474sSC6oe7A",
        "outputId": "8a2a25db-aa92-423b-e73b-6033f7be4741"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1b0dd29367694dc7939f1a906ba58424",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)ve/main/spiece.model:   0%|          | 0.00/798k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6148540e8507424bb3be6608447e95cd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)lve/main/config.json:   0%|          | 0.00/760 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[['▁a', '▁little', '▁less', '▁than', '▁a', '▁decade', '▁ago', '▁hockey', '▁fans', '▁were', '▁blessed', '▁with', '▁a', '▁slate', '▁of', '▁games', '▁every', '▁night', '▁but', '▁on', '▁', 'th', 'ur', 's', 'day', '▁sources', '▁confirmed', '▁that', '▁for', '▁the', '▁ninth', '▁consecutive', '▁year', '▁', 'n', 'hl', '▁players', '▁have', '▁been', '▁locked', '▁out', '▁with', '▁very', '▁slim', '▁hopes', '▁of', '▁an', '▁agreement', '▁in', '▁sight', '▁it', '▁seems', '▁like', '▁just', '▁yesterday', '▁', 'mar', 'tin', '▁', 'st', '▁', 'lou', 'is', '▁and', '▁his', '▁lightning', '▁teammates', '▁were', '▁raising', '▁the', '▁', 'stan', 'ley', '▁cup', '▁high', '▁school', '▁hockey', '▁coach', '▁and', '▁one', 'time', '▁', 'es', 'p', 'n', '▁analyst', '▁bar', 'ry', '▁', 'mel', 'rose', '▁said', '▁obviously', '▁im', '▁still', '▁hoping', '▁the', '▁two', '▁sides', '▁can', '▁come', '▁together', '▁and', '▁reach', '▁an', '▁agreement', '▁but', '▁im', '▁starting', '▁to', '▁think', '▁nobody', '▁really', '▁misses', '▁hockey', '▁anymore', '▁no', 'pe', '▁nobody', '▁but', '▁old', '▁bar', 'ry', '▁', 'id', '▁still', '▁love', '▁to', '▁catch', '▁an', '▁at', 'lan', 'ta', '▁', 'th', 'rash', 'ers', '▁game', '▁observers', '▁have', '▁noted', '▁that', '▁when', '▁arena', '▁doors', '▁do', '▁reopen', '▁the', '▁', 'n', 'hl', '▁will', '▁face', '▁the', '▁perhaps', '▁greater', '▁challenge', '▁of', '▁convincing', '▁fans', '▁to', '▁return', '▁to', '▁hockey', '▁instead', '▁of', '▁watching', '▁more', '▁popular', '▁sports', '▁like', '▁football', '▁basketball', '▁baseball', '▁and', '▁slam', 'ball', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']'], ['▁the', '▁writers', '▁of', '▁the', '▁', 'h', 'bo', '▁series', '▁the', '▁so', 'pra', 'no', 's', '▁took', '▁another', '▁daring', '▁storytelling', '▁step', '▁by', '▁killing', '▁off', '▁10', '▁million', '▁fans', '▁during', '▁the', '▁seventh', '▁seasons', '▁premiere', '▁episode', '▁sun', 'day', '▁night', '▁this', '▁was', '▁definitely', '▁a', '▁bold', '▁choice', '▁one', '▁that', '▁producers', '▁of', '▁the', '▁show', '▁would', '▁have', '▁never', '▁thought', '▁of', '▁making', '▁five', '▁years', '▁ago', '▁said', '▁new', '▁york', '▁times', '▁television', '▁critic', '▁', 'virginia', '▁he', 'ffer', 'nan', '▁who', '▁noted', '▁that', '▁the', '▁move', '▁was', '▁hinted', '▁at', '▁in', '▁a', '▁season', 'five', '▁episode', '▁in', '▁which', '▁to', 'ny', '▁dream', 't', '▁he', '▁was', '▁riding', '▁a', '▁horse', '▁through', '▁his', '▁house', '▁but', '▁now', '▁that', '▁', 'i', '▁look', '▁back', '▁this', '▁was', '▁strongly', '▁for', 'es', 'had', 'owed', '▁throughout', '▁all', '▁of', '▁last', '▁season', '▁industry', '▁insider', 's', '▁predicted', '▁that', '▁the', '▁shows', '▁producers', '▁would', '▁try', '▁to', '▁bring', '▁at', '▁least', '▁some', '▁fans', '▁back', '▁for', '▁the', '▁series', '▁finale', '▁which', '▁may', '▁come', '▁as', '▁early', '▁as', '▁may', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']'], ['▁despite', '▁claims', '▁from', '▁the', '▁', 'tv', '▁news', '▁outlet', '▁to', '▁offer', '▁nonstop', '▁news', '▁and', '▁coverage', '▁you', '▁can', '▁count', '▁on', '▁an', '▁self', 'news', '▁investigation', '▁has', '▁uncovered', '▁hundreds', '▁of', '▁instances', '▁in', '▁which', '▁', 'ka', 'm', 'r', '▁channel', '▁4', '▁10', '▁', 'o', 'clock', '▁eyewitness', '▁news', '▁team', '▁relied', '▁almost', '▁exclusively', '▁on', '▁news', '▁reports', '▁weather', '▁forecasts', '▁and', '▁even', '▁special', 'interest', '▁features', '▁already', '▁generated', '▁by', '▁the', '▁stations', '▁6', '▁', 'o', 'clock', '▁eyewitness', '▁news', '▁team', '▁the', '▁investigation', '▁found', '▁that', '▁10', '▁', 'o', 'clock', '▁news', '▁team', '▁is', '▁in', '▁fact', '▁not', '▁the', '▁team', '▁you', '▁can', '▁trust', '▁in', '▁an', '▁examination', '▁of', '▁98', '▁consecutive', '▁prime', 'time', '▁and', '▁late', 'night', '▁broadcasts', '▁including', '▁dozens', '▁more', '▁nationwide', '▁the', '▁a', 'mar', 'illo', 'based', '▁station', 'the', '▁regions', '▁self', 'styled', '▁news', '▁leader', 're', 'pe', 'ated', 'ly', '▁ran', '▁pieces', '▁for', '▁its', '▁health', '▁beat', '▁pet', '▁patrol', '▁and', '▁bargain', '▁', 'buster', 's', '▁segments', '▁in', '▁both', '▁evening', '▁news', '▁slots', '▁and', '▁regularly', '▁relay', 'ed', '▁the', '▁same', '▁weather', '▁updates', '▁and', '▁traffic', '▁reports', '▁up', '▁to', '▁15', '▁times', '▁a', '▁day', '▁', 'ka', 'm', 'r', '▁even', '▁routinely', '▁', 're', 'ha', 'shed', '▁6', '▁pm', '▁footage', '▁for', '▁seemingly', '▁urgent', '▁breaking', '▁news', '▁reports', '▁most', '▁recently', '▁the', '▁plum', '▁creek', '▁nursing', '▁home', '▁power', '▁out', 'age', '▁and', '▁the', '▁', 'bon', 'ham', '▁middle', '▁school', '▁roof', '▁collapse', '▁in', '▁an', '▁a', 'pri', 'l', '▁incident', '▁involving', '▁the', '▁10', '▁pm', '▁', 're', 'cap', '▁of', '▁a', '▁local', '▁cancer', '▁fun', '▁run', '▁anchor', '▁and', 'y', '▁just', 'us', '▁read', '▁almost', '▁the', '▁exact', '▁same', '▁copy', '▁introducing', '▁the', '▁piece', '▁as', '▁he', '▁had', '▁just', '▁four', '▁hours', '▁earlier', '▁while', '▁reporter', '▁', 'sha', 'land', 'y', 's', '▁and', 'erson', '▁altered', '▁only', '▁one', '▁word', '▁between', '▁broadcasts', '▁changing', '▁heart', 'war', 'ming', '▁to', '▁in', 'spir', 'ing', '▁if', '▁they', 're', '▁on', '▁our', '▁side', '▁as', '▁they', '▁claim', '▁what', '▁then', '▁is', '▁a', '▁purported', 'ly', '▁professional', '▁news', '▁team', '▁doing', '▁in', '▁the', '▁four', '▁hours', '▁between', '▁broadcasts', '▁a', 'mar', 'illo', '▁resident', '▁and', '▁frequent', '▁local', 'news', '▁viewer', '▁mark', '▁jet', 'te', '▁said', '▁during', '▁another', '▁10', '▁pm', '▁broadcast', '▁live', '▁continuing', '▁coverage', '▁from', '▁reporter', '▁mat', 't', '▁or', 'land', 'o', '▁of', '▁a', '▁two', 'al', 'arm', '▁', 'el', 'wood', '▁park', '▁house', '▁fire', '▁consisted', '▁almost', '▁wholly', '▁of', '▁previously', '▁aired', '▁footage', '▁of', '▁the', '▁firefighters', '▁in', '▁action', '▁the', '▁lack', '▁of', '▁updated', '▁footage', '▁disappointed', '▁viewers', '▁such', '▁as', '▁here', 'ford', '▁tx', 's', '▁', 'kel', 'ly', '▁by', 'er', '▁whose', '▁mild', '▁curiosity', '▁about', '▁the', '▁blaze', '▁first', '▁p', 'ique', 'd', '▁at', '▁the', '▁6', '▁pm', '▁newscast', '▁went', '▁un', 'grat', 'ified', '▁', 'h', '3', 'well', '▁continue', '▁to', '▁watch', '▁this', '▁important', '▁breaking', '▁news', '▁story', 'h', '3', 'pel', 'iza', 'be', 'th', '▁', 'din', 'h', '▁after', '▁a', '▁report', '▁on', '▁a', '▁broken', '▁gas', '▁main', '▁which', '▁had', '▁already', '▁run', '▁in', '▁two', '▁previous', '▁newscast', 'sp', '▁its', '▁true', '▁that', '▁the', '▁image', '▁of', '▁that', '▁', 's', 'cor', 'ched', '▁little', '▁doll', '▁was', '▁powerful', '▁and', '▁may', '▁have', '▁bore', '▁repeating', '▁but', '▁where', '▁was', '▁the', '▁follow', 'up', '▁footage', '▁of', '▁the', '▁devastated', '▁family', '▁at', '▁a', '▁red', '▁cross', '▁shelter', '▁by', 'er', '▁said', '▁or', '▁some', '▁fresh', '▁', 'bro', 'll', '▁of', '▁the', '▁charred', '▁ruins', '▁of', '▁the', '▁house', '▁the', '▁public', '▁deserves', '▁better', '▁the', '▁investigation', '▁also', '▁found', '▁the', '▁10', '▁pm', '▁', 'ka', 'm', 'r', '▁broadcast', '▁consistently', '▁', 're', 'air', 'ed', '▁closing', '▁stock', '▁numbers', '▁and', '▁high', 'school', '▁baseball', '▁highlights', '▁and', '▁sports', '▁', 'blo', 'oper', 's', '▁its', '▁producers', '▁and', '▁anchor', 's', '▁apparently', '▁unaware', '▁or', '▁in', 'different', '▁to', '▁the', '▁fact', '▁that', '▁the', '▁information', '▁was', '▁hours', '▁old', '▁and', '▁already', '▁common', '▁knowledge', '▁among', '▁viewers', '▁they', '▁say', '▁that', '▁the', '▁6', '▁', 'o', 'clock', '▁news', '▁team', '▁is', '▁the', '▁areas', '▁most', '▁watched', '▁news', '▁team', '▁jet', 'te', '▁said', '▁especially', '▁by', '▁the', '▁10', '▁', 'o', 'clock', '▁news', '▁team', '▁just', '▁last', '▁', 'tu', 'es', 'day', '▁investigative', '▁reporter', '▁me', 'a', 'ghan', '▁coll', 'ier', 's', '▁problem', '▁solve', 'rs', '▁segment', '▁on', '▁', 's', 'qual', 'id', '▁conditions', '▁at', '▁a', '▁local', '▁dog', '▁', 'ken', 'nel', '▁aired', '▁again', '▁at', '▁10', '▁pm', '▁without', '▁even', '▁a', '▁cursor', 'y', '▁update', '▁on', '▁the', '▁broken', 'legged', '▁puppy', '▁featured', '▁in', '▁the', '▁report', '▁their', '▁', 's', 'cor', 'ching', '▁summer', '▁coverage', '▁was', '▁even', '▁worse', '▁jet', 'te', '▁continued', '▁how', '▁many', '▁times', '▁do', '▁you', '▁have', '▁to', '▁repeat', '▁the', '▁same', '▁cool', '▁tips', '▁before', '▁all', '▁of', '▁a', 'mar', 'illo', '▁is', '▁crystal', '▁clear', '▁on', '▁exactly', '▁how', '▁to', '▁beat', '▁the', '▁heat', '▁while', '▁', 'ka', 'm', 'r', '▁was', '▁a', '▁particularly', '▁flag', 'rant', '▁offender', '▁it', '▁is', '▁by', '▁no', '▁means', '▁alone', '▁in', '▁a', '▁segment', '▁about', '▁the', '▁', 'san', '▁die', 'go', '▁zoo', 's', '▁baby', '▁panda', 's', '▁', 'k', 'f', 'mb', '▁', 'tv', '8', 's', '▁news', '▁at', '▁11', '▁not', '▁only', '▁offered', '▁footage', '▁identical', '▁to', '▁the', '▁previous', '▁telecast', '▁but', '▁practically', '▁in', 'distinguishable', '▁co', 'os', '▁of', '▁affection', '▁from', '▁the', '▁co', 'an', 'chor', '▁at', '▁some', '▁stations', '▁the', '▁problem', '▁goes', '▁far', '▁beyond', '▁one', 'time', '▁reuse', '▁an', '▁11', '▁pm', '▁segment', '▁on', '▁heart', 'smart', '▁dinner', '▁alternatives', '▁on', '▁new', '▁haven', '▁', 'connecticut', 's', '▁', 'w', 't', 'nh', '▁channel', '▁8', '▁was', '▁not', '▁only', '▁previously', '▁seen', '▁on', '▁the', '▁6', '▁pm', '▁news', '▁but', '▁also', '▁on', '▁live', '▁at', '▁5', '▁the', '▁4', '▁report', '▁and', '▁the', '▁news', '▁at', '▁noon', '▁with', '▁', 's', 'onia', '▁bag', 'h', 'da', 'dy', '▁another', '▁piece', '▁on', '▁the', '▁city', 's', '▁aging', '▁school', '▁buses', '▁was', '▁rotate', 'd', '▁into', '▁the', '▁following', '▁days', '▁good', '▁morning', '▁new', '▁haven', '▁as', '▁well', '▁in', '▁extreme', '▁examples', '▁such', '▁as', '▁in', '▁', 'lou', 'is', 'ville', '▁', 'ky', 's', '▁', 'wl', 'ky', 'tv', '▁prime', 'time', '▁and', '▁late', 'night', '▁newscast', 's', '▁the', '▁only', '▁', 'distinguishable', '▁characteristic', '▁is', '▁the', '▁lead', '▁anchor', 's', '▁concluding', '▁suggestion', '▁to', '▁stay', '▁tune', 'd', '▁for', '▁da', 'vid', '▁letter', 'man', 's', '▁the', '▁late', '▁show', '▁despite', '▁the', '▁mounting', '▁controversy', '▁over', '▁the', '▁', 'ka', 'm', 'r', '▁channel', '▁4', '▁team', '▁the', '▁investigation', '▁was', '▁unable', '▁to', '▁conclusive', 'ly', '▁prove', '▁that', '▁the', '▁hopeful', '▁wishes', '▁to', '▁see', '▁viewers', '▁the', '▁next', '▁day', '▁and', '▁the', '▁ca', 'mara', 'der', 'ie', '▁and', '▁laughter', '▁shared', '▁between', '▁co', 'an', 'chor', 's', '▁', 'ky', 'la', '▁', 'cul', 'lin', 'ane', '▁and', '▁', 'el', 'iza', 'be', 'th', '▁', 'din', 'h', '▁were', '▁anything', '▁less', '▁than', '▁genuine', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']'], ['▁after', '▁receiving', '▁sub', 'par', '▁service', '▁and', '▁experiencing', '▁an', '▁unusually', '▁long', '▁wait', '▁for', '▁his', '▁4', '75', '▁lunch', '▁at', '▁a', '▁local', '▁beef', 'side', '▁family', '▁restaurant', '▁', 'mon', 'day', '▁customer', '▁', 'gus', '▁', 'o', 'con', 'nor', '▁opted', '▁to', '▁give', '▁waitress', '▁car', 'la', '▁', 'hy', 'am', 's', '▁a', '▁reduced', '▁10', '▁percent', '▁tip', '▁in', '▁an', '▁attempt', '▁to', '▁communicate', '▁his', '▁dissatisfaction', '▁and', '▁raise', '▁awareness', '▁of', '▁the', '▁areas', '▁in', '▁which', '▁he', '▁felt', '▁her', '▁performance', '▁was', '▁lacking', '▁', 'o', 'con', 'nor', '▁hoped', '▁his', '▁reduced', '▁tip', '▁would', '▁be', '▁a', '▁wake', 'up', '▁call', '▁for', '▁', 'hy', 'am', 's', '▁', 'hy', 'am', 's', '▁49', '▁who', '▁has', '▁been', '▁serving', '▁tables', '▁at', '▁the', '▁popular', '▁eat', 'ery', '▁for', '▁13', '▁years', '▁expressed', '▁enthusiastic', '▁gratitude', '▁for', '▁the', '▁immense', '▁personal', '▁growth', '▁the', '▁gesture', '▁will', '▁afford', '▁her', '▁adding', '▁that', '▁in', '▁the', '▁long', '▁run', '▁the', '▁experience', '▁will', '▁make', '▁her', '▁a', '▁better', '▁waitress', '▁maybe', '▁', 'i', '▁was', '▁a', '▁little', '▁short', '▁with', '▁him', '▁when', '▁', 'i', '▁told', '▁him', '▁to', '▁hold', '▁on', '▁a', '▁', 'sec', '▁but', '▁in', '▁the', '▁future', '▁ill', '▁do', '▁my', '▁best', '▁to', '▁ensure', '▁a', '▁situation', '▁like', '▁that', '▁never', '▁ever', '▁happens', '▁again', '▁said', '▁', 'hy', 'am', 's', '▁who', '▁put', '▁', 'o', 'con', 'nor', 's', '▁order', '▁slip', '▁in', '▁as', '▁the', '▁under', 'staff', 'ed', '▁cook', 's', '▁dealt', '▁with', '▁a', '▁large', '▁complicated', '▁meal', '▁for', '▁a', '▁bus', 'load', '▁of', '▁senior', 'citizen', '▁tourists', '▁its', '▁days', '▁like', '▁this', '▁that', '▁', 'i', '▁thank', '▁god', '▁', 'i', '▁get', '▁paid', '▁less', '▁than', '▁minimum', '▁wage', '▁and', '▁can', '▁rely', '▁on', '▁a', '▁built', 'in', '▁economic', '▁incentive', '▁to', '▁keep', '▁me', '▁motivated', '▁during', '▁those', '▁16', 'hour', '▁double', '▁shifts', '▁', 'hy', 'am', 's', '▁added', '▁that', '▁she', '▁now', '▁knows', '▁she', '▁should', '▁always', '▁bring', '▁a', '▁glass', '▁of', '▁water', '▁without', '▁any', '▁ice', '▁cubes', '▁every', '▁time', '▁someone', '▁orders', '▁a', '▁diet', '▁co', 'ke', '▁and', '▁that', '▁the', '▁phrase', '▁when', '▁you', '▁get', '▁a', '▁minute', '▁is', '▁in', '▁fact', '▁a', '▁polite', '▁way', '▁of', '▁indicating', '▁that', '▁the', '▁customer', '▁wants', '▁his', '▁request', '▁filled', '▁in', '▁under', '▁one', '▁minute', '▁if', '▁he', '▁hadn', 't', '▁with', 'held', '▁that', '▁50', '▁cents', '▁', 'id', '▁make', '▁these', '▁same', '▁mistakes', '▁over', '▁and', '▁over', '▁for', '▁the', '▁rest', '▁of', '▁my', '▁career', '▁she', '▁said', '▁even', '▁at', '▁my', '▁age', '▁its', '▁amazing', '▁to', '▁think', '▁you', '▁can', '▁still', '▁learn', '▁something', '▁new', '▁about', '▁a', '▁low', 'paying', '▁men', 'ial', 'labor', '▁job', '▁', 'hy', 'am', 's', '▁added', '▁that', '▁the', '▁next', '▁time', '▁she', '▁sees', '▁', 'o', 'con', 'nor', '▁she', '▁will', '▁remember', '▁that', '▁he', '▁under', 't', 'ipped', '▁her', '▁and', '▁strive', '▁to', '▁serve', '▁him', '▁better', '▁to', '▁avoid', '▁any', '▁further', '▁disappointment', '▁', 'o', 'con', 'nor', '▁he', '▁may', '▁not', '▁realize', '▁it', '▁but', '▁his', '▁actions', '▁today', '▁will', '▁not', '▁only', '▁improve', '▁my', '▁work', '▁ethic', '▁but', '▁will', '▁directly', '▁benefit', '▁him', '▁as', '▁well', '▁in', '▁that', '▁', 'i', '▁will', '▁gain', '▁economic', '▁and', '▁personal', '▁rewards', '▁by', '▁treating', '▁him', '▁with', '▁the', '▁tremendous', '▁respect', '▁and', '▁un', 'fail', 'ing', '▁attention', '▁he', '▁deserves', '▁', 'hy', 'am', 's', '▁said', '▁so', '▁really', '▁if', '▁you', '▁think', '▁about', '▁it', '▁that', '▁10', '▁percent', '▁tip', '▁is', '▁a', '▁win', 'win', '▁situation', '▁for', '▁both', '▁of', '▁us', '▁', 'o', 'con', 'nor', '▁said', '▁he', '▁felt', '▁he', '▁needed', '▁to', '▁get', '▁through', '▁to', '▁the', '▁waitress', '▁and', '▁did', '▁so', '▁the', '▁best', '▁way', '▁he', '▁knew', '▁how', '▁by', '▁giving', '▁her', '▁less', '▁than', '▁the', '▁universally', '▁agreed', 'upon', '▁minimum', '▁', 'i', '▁sent', '▁a', '▁clear', '▁unmistakable', '▁yet', '▁constructive', '▁message', '▁said', '▁', 'o', 'con', 'nor', '▁who', '▁claimed', '▁that', '▁he', '▁hoped', '▁the', '▁smaller', '▁tip', '▁would', '▁be', '▁a', '▁wake', 'up', '▁call', '▁for', '▁', 'hy', 'am', 's', '▁', 'i', '▁was', '▁just', '▁trying', '▁to', '▁help', '▁push', '▁car', 'la', '▁along', '▁the', '▁path', '▁to', '▁achieving', '▁her', '▁full', '▁potential', '▁as', '▁an', '▁employee', '▁it', '▁was', '▁the', '▁absolute', '▁least', '▁', 'i', '▁could', '▁do', '▁he', '▁added', '▁', 'o', 'con', 'nor', '▁said', '▁he', '▁first', '▁considered', '▁reducing', '▁his', '▁usual', '▁15', '▁percent', '▁tip', '▁for', '▁the', '▁waitress', '▁when', '▁', 'hy', 'am', 's', '▁failed', '▁to', '▁replace', '▁the', '▁cream', '▁packet', 's', '▁for', '▁his', '▁coffee', '▁while', '▁he', '▁looked', '▁over', '▁the', '▁restaurants', '▁extensive', '▁list', '▁of', '▁lunch', '▁special', 's', '▁but', '▁it', '▁wasn', 't', '▁until', '▁', 'hy', 'am', 's', '▁neglected', '▁to', '▁ask', '▁if', '▁he', '▁needed', '▁extra', '▁', 'ke', 'tch', 'up', '▁that', '▁', 'o', 'con', 'nor', '▁made', '▁the', '▁decision', '▁to', '▁let', '▁his', '▁money', '▁do', '▁the', '▁talking', '▁in', '▁the', '▁competitive', '▁service', '▁industry', '▁there', '▁is', '▁a', '▁mechanism', '▁to', '▁effect', '▁change', '▁he', '▁said', '▁', 'i', '▁know', '▁this', '▁will', '▁be', '▁an', '▁invaluable', '▁lesson', '▁she', '▁won', 't', '▁soon', '▁forget', '▁but', '▁', 'i', '▁just', '▁did', '▁what', '▁any', '▁decent', '▁human', '▁being', '▁in', '▁my', '▁position', '▁would', '▁have', '▁done', '▁and', '▁that', '▁feels', '▁good', '▁', 'o', 'con', 'nor', '▁said', '▁his', '▁overall', '▁goal', '▁was', '▁not', '▁only', '▁to', '▁receive', '▁better', '▁service', '▁but', '▁to', '▁help', '▁', 'hy', 'am', 's', '▁become', '▁a', '▁role', '▁model', '▁for', '▁her', '▁two', '▁teenage', '▁children', '▁', 'ty', 'ler', '▁and', '▁mi', 'cha', 'el', '▁', 'i', '▁know', '▁as', '▁well', '▁as', '▁anyone', '▁how', '▁hard', '▁it', '▁is', '▁for', '▁a', '▁single', '▁mother', '▁with', '▁a', '▁limited', '▁income', '▁to', '▁raise', '▁kids', '▁on', '▁her', '▁own', '▁he', '▁said', '▁but', '▁this', '▁way', '▁they', '▁learn', '▁the', '▁value', '▁of', '▁money', '▁and', '▁the', '▁satisfaction', '▁of', '▁a', '▁job', '▁well', '▁done', '▁in', '▁the', '▁end', '▁', 'hy', 'am', 's', '▁said', '▁she', '▁could', '▁not', '▁agree', '▁more', '▁my', '▁boys', '▁have', '▁had', '▁a', '▁few', '▁run', 'ins', '▁with', '▁the', '▁law', '▁and', '▁they', '▁could', '▁certainly', '▁use', '▁some', '▁good', '▁advice', '▁she', '▁said', '▁', 'i', '▁can', 't', '▁wait', '▁for', '▁them', '▁and', '▁maybe', '▁a', '▁couple', '▁of', '▁their', '▁friends', '▁to', '▁meet', '▁', 'm', 'r', '▁', 'o', 'con', 'nor', '▁first', 'hand', '▁', 'i', '▁think', '▁they', 'd', '▁get', '▁a', '▁lot', '▁out', '▁of', '▁it', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']'], ['▁after', '▁watching', '▁his', '▁beloved', '▁seat', 'tle', '▁marine', 'rs', '▁prevail', '▁against', '▁the', '▁', 'san', '▁die', 'go', '▁pad', 're', 's', '▁third', 'grader', '▁', 'tim', 'my', '▁has', 'ter', 't', '▁was', '▁moved', '▁to', '▁ask', '▁his', '▁father', '▁46', 'year', 'old', '▁insurance', '▁salesman', '▁', 'christ', 'oph', 'er', '▁has', 'ter', 't', '▁why', '▁inter', 'league', '▁play', '▁is', '▁good', '▁well', '▁it', '▁lets', '▁people', '▁see', '▁the', '▁teams', '▁they', '▁normally', '▁don', 't', '▁get', '▁to', '▁see', '▁all', '▁that', '▁often', '▁', 'i', '▁think', '▁is', '▁the', '▁point', '▁there', '▁buddy', '▁has', 'ter', 't', '▁said', '▁after', '▁beginning', '▁three', '▁different', '▁sentences', '▁in', '▁seven', '▁minutes', '▁after', '▁all', '▁without', '▁inter', 'league', '▁play', '▁we', '▁wouldn', 't', '▁get', '▁to', '▁see', '▁players', '▁like', '▁like', '▁', 'bri', 'an', '▁', 'gil', 'es', '▁and', '▁', 's', 'cott', '▁line', 'b', 'rink', '▁would', '▁we', '▁although', '▁', 'i', '▁think', '▁we', '▁play', '▁the', '▁yank', 'ees', '▁and', '▁red', '▁so', 'x', '▁less', '▁often', '▁as', '▁a', '▁result', '▁right', '▁yeah', '▁im', '▁pretty', '▁sure', '▁that', 's', '▁how', '▁they', '▁do', '▁it', '▁in', '▁a', '▁moving', '▁but', '▁ultimately', '▁doomed', '▁effort', '▁to', '▁give', '▁his', '▁impression', 'able', '▁boy', '▁the', '▁right', '▁messages', '▁', 'christ', 'oph', 'er', '▁also', '▁attempted', '▁to', '▁answer', '▁', 'tim', 'my', 's', '▁questions', '▁regarding', '▁why', '▁only', '▁the', '▁', 'american', '▁league', '▁has', '▁a', '▁', 'dh', '▁why', '▁and', '▁how', '▁the', '▁all', 'star', '▁game', '▁now', '▁counts', '▁what', '▁performance', 'enhancing', '▁drugs', '▁are', '▁and', '▁how', '▁baseball', '▁officials', '▁could', '▁have', '▁sat', '▁', 'id', 'ly', '▁by', '▁when', '▁they', '▁knew', '▁there', '▁was', '▁a', '▁major', '▁steroid', '▁problem', '▁in', '▁their', '▁sport', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']'], ['▁at', '▁a', '▁cafeteria', 'table', '▁press', '▁conference', '▁', 'mon', 'day', '▁da', 'vid', '▁per', 'nell', '▁10', '▁categori', 'cal', 'ly', '▁denied', '▁girl', 'lik', 'ing', '▁allegations', '▁recently', '▁levied', '▁against', '▁him', '▁by', '▁fellow', '▁lake', 'view', '▁elementary', '▁school', '▁fourth', 'grader', '▁', 'jo', 'na', 'than', '▁', 'witt', '▁', 'i', '▁do', '▁not', '▁have', '▁not', '▁and', '▁will', '▁not', '▁ever', '▁like', '▁girls', '▁per', 'nell', '▁told', '▁the', '▁crowd', '▁of', '▁seven', '▁boys', '▁assembled', '▁at', '▁the', '▁lunch', 'room', 's', '▁back', '▁table', '▁', 'm', 'r', '▁', 'witt', 's', '▁accusations', '▁are', '▁not', '▁only', '▁100', '▁percent', '▁false', '▁but', '▁also', '▁', 's', 'lander', 'ous', '▁as', '▁it', '▁has', '▁always', '▁been', '▁my', '▁firm', '▁conviction', '▁that', '▁girls', '▁are', '▁totally', '▁and', '▁completely', '▁gross', '▁per', 'nell', '▁went', '▁on', '▁to', '▁suggest', '▁that', '▁perhaps', '▁it', '▁is', '▁', 'witt', '▁who', '▁like', 's', '▁girls', '▁particularly', '▁', 'je', 'nny', '▁', 'lough', 'lin', '▁10', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']'], ['▁stunned', '▁shock', '▁and', '▁dismay', '▁were', '▁just', '▁a', '▁few', '▁of', '▁the', '▁reactions', '▁from', '▁bob', 'by', '▁gun', 'ter', 'grass', '▁on', '▁', 'tu', 'es', 'day', '▁when', '▁the', '▁10', 'year', 'old', '▁learned', '▁that', '▁the', '▁woman', '▁from', '▁the', '▁', 'guin', 'ness', '▁book', '▁of', '▁world', '▁records', '▁who', '▁has', '▁the', '▁ability', '▁to', '▁pop', '▁her', '▁', 'bul', 'ging', '▁eye', 'ball', 's', '▁nearly', '▁all', '▁the', '▁way', '▁out', '▁of', '▁their', '▁socket', 's', '▁was', '▁not', '▁a', '▁million', 'aire', '▁no', '▁way', '▁she', 's', '▁gotta', '▁to', '▁be', '▁rich', '▁said', '▁gun', 'ter', 'grass', '▁adding', '▁that', '▁if', '▁he', '▁himself', '▁possessed', '▁the', '▁woman', 's', '▁unique', '▁attribute', '▁he', '▁would', '▁be', '▁a', '▁billionaire', '▁maybe', '▁she', '▁spent', '▁it', '▁all', '▁on', '▁a', '▁giant', '▁house', '▁yeah', '▁that', '▁makes', '▁sense', '▁she', '▁probably', '▁bought', '▁a', '▁huge', '▁mansion', '▁at', '▁press', '▁time', '▁gun', 'ter', 'grass', '▁had', '▁reportedly', '▁grown', '▁even', '▁more', '▁disillusioned', '▁after', '▁learning', '▁that', '▁the', '▁man', '▁who', '▁holds', '▁the', '▁record', '▁for', '▁the', '▁world', 's', '▁longest', '▁fingernails', '▁lives', '▁in', '▁a', '▁developing', '▁nation', '▁and', '▁has', '▁no', '▁access', '▁to', '▁clean', '▁water', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']'], ['▁speaking', '▁with', '▁reporters', '▁before', '▁a', '▁game', '▁', 'mon', 'day', '▁local', '▁little', '▁league', 'r', '▁', 'na', 'than', '▁', 'gar', 'r', 'ett', '▁expressed', '▁his', '▁heartfelt', '▁wish', '▁that', '▁just', '▁once', '▁his', '▁unemployed', '▁father', '▁could', '▁not', '▁make', '▁it', '▁out', '▁to', '▁see', '▁him', '▁play', '▁every', '▁time', '▁', 'i', '▁come', '▁up', '▁to', '▁bat', '▁', 'i', '▁hope', '▁deep', '▁down', '▁inside', '▁that', '▁this', '▁will', '▁be', '▁the', '▁time', '▁that', '▁', 'i', '▁look', '▁up', '▁into', '▁the', '▁stands', '▁and', '▁don', 't', '▁see', '▁my', '▁dad', '▁said', '▁the', '▁visibly', '▁down', 'cast', '▁10', 'year', 'old', '▁confirming', '▁that', '▁his', '▁father', '▁pet', 'er', '▁', 'gar', 'r', 'ett', '▁a', '▁former', '▁hospital', '▁records', '▁clerk', '▁who', '▁has', '▁been', '▁out', '▁of', '▁work', '▁for', '▁15', '▁months', '▁had', '▁not', '▁missed', '▁even', '▁one', '▁game', '▁all', '▁season', '▁', 'i', '▁look', '▁down', '▁the', '▁rows', '▁of', '▁bleach', 'ers', '▁but', '▁he', 's', '▁always', '▁there', '▁and', '▁when', '▁', 'i', '▁see', '▁him', '▁my', '▁heart', '▁just', '▁sink', 's', '▁', 'i', '▁would', '▁give', '▁anything', '▁for', '▁him', '▁to', '▁be', '▁in', '▁an', '▁office', '▁not', '▁watching', '▁me', '▁play', '▁', 'gar', 'r', 'ett', '▁added', '▁the', '▁fourth', 'grader', '▁admitted', '▁he', '▁often', '▁feels', '▁', 'en', 'vi', 'ous', '▁of', '▁his', '▁teammates', '▁whose', '▁steadily', '▁employed', '▁father', 's', '▁are', '▁either', '▁unable', '▁to', '▁make', '▁it', '▁to', '▁games', '▁or', '▁only', '▁show', '▁up', '▁for', '▁the', '▁final', '▁inning', '▁or', '▁two', '▁in', '▁addition', '▁', 'gar', 'r', 'ett', '▁reported', '▁being', '▁especially', '▁saddened', '▁when', '▁his', '▁dad', '▁failed', '▁to', '▁miss', '▁even', '▁the', '▁most', '▁inconvenient', 'ly', '▁time', 'd', '▁games', '▁right', '▁after', '▁school', '▁in', '▁the', '▁middle', '▁of', '▁the', '▁week', '▁stating', '▁that', '▁he', '▁felt', '▁embarrassed', '▁and', '▁ashamed', '▁that', '▁everyone', '▁else', '▁on', '▁the', '▁team', '▁might', '▁notice', '▁that', '▁his', '▁father', '▁was', '▁the', '▁only', '▁parent', '▁in', '▁the', '▁stands', '▁with', '▁his', '▁father', '▁never', '▁putting', '▁in', '▁long', '▁hours', '▁at', '▁work', '▁', 'gar', 'r', 'ett', '▁acknowledged', '▁that', '▁he', '▁has', '▁devised', '▁ways', '▁to', '▁help', '▁him', '▁cope', '▁with', '▁the', '▁constant', '▁attention', '▁he', '▁receives', '▁', 'i', '▁try', '▁to', '▁put', '▁the', '▁whole', '▁thing', '▁out', '▁of', '▁my', '▁mind', '▁but', '▁sometimes', '▁its', '▁really', '▁hard', '▁not', '▁to', '▁think', '▁about', '▁how', '▁dad', '▁isn', 't', '▁stuck', '▁behind', '▁a', '▁desk', '▁somewhere', '▁especially', '▁when', '▁he', '▁stands', '▁up', '▁and', '▁cheer', 's', '▁my', '▁name', '▁said', '▁', 'gar', 'r', 'ett', '▁', 's', 'ul', 'len', 'ly', '▁noting', '▁that', '▁the', '▁number', '▁of', '▁games', '▁left', '▁in', '▁the', '▁season', '▁that', '▁his', '▁father', '▁could', '▁potentially', '▁miss', '▁were', '▁rapidly', '▁running', '▁out', '▁and', '▁once', '▁', 'i', '▁start', '▁thinking', '▁about', '▁how', '▁he', '▁came', '▁to', '▁yet', '▁another', '▁game', '▁and', '▁how', '▁hell', '▁probably', '▁come', '▁to', '▁the', '▁next', '▁one', '▁too', '▁', 'i', '▁just', '▁feel', '▁awful', '▁', 'i', '▁guess', '▁dad', '▁will', '▁always', '▁be', '▁the', '▁one', '▁wh', 'ist', 'ling', '▁and', '▁clap', 'ping', '▁for', '▁me', '▁in', '▁the', '▁stands', '▁instead', '▁of', '▁further', 'ing', '▁a', '▁career', '▁', 'gar', 'r', 'ett', '▁continued', '▁and', '▁that', 's', '▁just', '▁something', '▁', 'i', '▁have', '▁to', '▁live', '▁with', '▁in', '▁addition', '▁to', '▁showing', '▁up', '▁at', '▁games', '▁', 'gar', 'r', 'ett', 's', '▁long', 'un', 'employed', '▁father', '▁is', '▁said', '▁to', '▁frequently', '▁disappoint', '▁his', '▁son', '▁with', '▁his', '▁constant', '▁availability', '▁at', '▁home', '▁as', '▁well', '▁in', '▁particular', '▁', 'gar', 'r', 'ett', '▁stated', '▁that', '▁his', '▁dad', '▁who', '▁hasn', 't', '▁had', '▁a', '▁job', '▁interview', '▁in', '▁months', '▁is', '▁always', '▁ready', '▁to', '▁play', '▁catch', '▁in', '▁the', '▁backyard', '▁at', '▁any', '▁time', '▁drive', '▁him', '▁to', '▁and', '▁from', '▁practice', '▁at', '▁all', '▁hours', '▁and', '▁even', '▁help', '▁out', '▁with', '▁school', '▁or', '▁boy', '▁scout', 's', '▁projects', '▁more', 'over', '▁', 'gar', 'r', 'ett', '▁noted', '▁with', '▁a', '▁de', 'ject', 'ed', '▁sigh', '▁that', '▁his', '▁father', '▁frequently', '▁offers', '▁to', '▁take', '▁him', '▁to', '▁the', '▁local', '▁minor', '▁league', '▁ballpark', '▁in', '▁spite', '▁of', '▁his', '▁frustration', 's', '▁with', '▁his', '▁father', 's', '▁behavior', '▁', 'gar', 'r', 'ett', '▁said', '▁that', '▁ever', '▁since', '▁he', '▁joined', '▁little', '▁league', '▁last', '▁spring', 'short', 'ly', '▁after', '▁his', '▁dad', '▁was', '▁laid', '▁off', '▁from', '▁the', '▁job', '▁he', 'd', '▁held', '▁for', '▁over', '▁a', '▁decade', 'he', '▁has', '▁clung', '▁to', '▁a', '▁sliver', '▁of', '▁hope', '▁that', '▁his', '▁father', '▁might', '▁one', '▁day', '▁apologize', '▁and', '▁offer', '▁an', '▁excuse', '▁about', '▁having', '▁important', '▁work', '▁to', '▁take', '▁care', '▁of', '▁instead', '▁of', '▁showing', '▁up', '▁', 'i', '▁just', '▁want', '▁dad', '▁to', '▁come', '▁up', '▁to', '▁me', '▁before', '▁a', '▁game', '▁put', '▁his', '▁hand', '▁on', '▁my', '▁shoulder', '▁and', '▁finally', '▁say', '▁im', '▁not', '▁going', '▁to', '▁be', '▁there', '▁tonight', '▁champ', '▁said', '▁the', '▁second', '▁baseman', '▁for', '▁the', '▁leagues', '▁blue', '▁', 'jay', 's', '▁club', '▁if', '▁he', '▁only', '▁knew', '▁how', '▁much', '▁', 'i', '▁think', '▁about', '▁walking', '▁off', '▁the', '▁field', '▁after', '▁a', '▁game', '▁without', '▁being', '▁hugged', '▁or', '▁congratulated', '▁that', 's', '▁all', '▁', 'i', '▁want', '▁ultimately', '▁', 'gar', 'r', 'ett', '▁said', '▁that', '▁when', '▁he', '▁later', '▁looks', '▁back', '▁at', '▁this', '▁time', '▁in', '▁his', '▁life', '▁he', '▁wants', '▁to', '▁remember', '▁his', '▁father', '▁having', '▁work', '▁obligations', '▁that', '▁prevented', '▁him', '▁from', '▁attending', '▁his', '▁sons', '▁extra', 'curricular', '▁activities', '▁', 'i', '▁just', '▁wish', '▁dad', '▁would', '▁realize', '▁how', '▁much', '▁it', '▁means', '▁to', '▁me', '▁that', '▁he', '▁not', '▁be', '▁here', '▁said', '▁', 'gar', 'r', 'ett', '▁before', '▁taking', '▁the', '▁field', '▁when', '▁it', '▁comes', '▁down', '▁to', '▁it', '▁', 'i', '▁just', '▁want', '▁my', '▁dad', '▁to', '▁have', '▁a', '▁little', '▁less', '▁time', '▁for', '▁me', '▁you', '▁know', '▁the', '▁10', 'year', 'old', '▁then', '▁reportedly', '▁covered', '▁the', '▁tears', '▁well', 'ing', '▁in', '▁his', '▁eyes', '▁with', '▁his', '▁glove', '▁after', '▁hearing', '▁the', '▁phrase', '▁go', '▁get', '▁em', '▁', 'nate', '▁shouted', '▁from', '▁behind', '▁the', '▁dugout', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']'], ['▁sports', '▁journalists', '▁and', '▁television', '▁crews', '▁were', '▁pushed', '▁aside', '▁during', '▁super', '▁bowl', '▁media', '▁day', '▁on', '▁', 'tu', 'es', 'day', '▁as', '▁more', '▁than', '▁1000', '▁writers', '▁for', '▁the', '▁website', '▁bleach', 'er', 'report', 'com', '▁entered', '▁', 'luc', 'as', '▁oil', '▁stadium', '▁to', '▁acquire', '▁material', '▁for', '▁their', '▁trademark', '▁style', '▁of', '▁report', 'age', '▁', 'i', '▁asked', '▁to', 'm', '▁bar', 'dy', '▁', 's', 'ic', '▁where', '▁he', '▁thought', '▁he', '▁should', '▁be', '▁on', '▁my', '▁list', '▁of', '▁the', '▁top', '▁10', '▁guys', '▁to', '▁ever', '▁play', '▁in', '▁the', '▁super', '▁bowl', '▁and', '▁he', '▁said', '▁it', '▁didn', 't', '▁sound', '▁like', '▁', 'i', '▁had', '▁anyone', '▁who', '▁played', '▁before', '▁2005', '▁said', '▁bleach', 'er', '▁report', '▁writer', '▁', 'dar', 'ron', '▁nasty', '▁16', '▁whose', '▁credentials', '▁identified', '▁him', '▁as', '▁writer', '▁top', '▁10', '▁', 'carolina', '▁and', '▁', 'miami', '▁hurricane', 's', '▁writer', '▁', 'jun', 'e', '▁2011', '▁', 'i', '▁think', '▁we', '▁got', '▁a', '▁lot', '▁of', '▁good', '▁answers', '▁though', '▁and', '▁our', '▁super', '▁bowl', '▁analysis', '▁lists', '▁are', '▁going', '▁to', '▁give', '▁a', '▁lot', '▁of', '▁insight', '▁to', '▁the', '▁fans', '▁and', '▁coaching', '▁staff', 's', '▁also', '▁at', '▁media', '▁day', '▁for', '▁the', '▁first', '▁time', '▁was', '▁the', '▁sports', '▁and', '▁culture', '▁web', '▁magazine', '▁grant', 'land', '▁which', '▁sent', '▁', 'vin', '▁', 's', 'cul', 'ly', '▁to', '▁', 'nar', 'rate', '▁the', '▁proceedings', '▁so', '▁that', '▁editors', '▁could', '▁sit', '▁at', '▁home', '▁listening', '▁to', '▁him', '▁on', '▁the', '▁radio', '▁while', '▁drinking', '▁', 'cog', 'nac', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']'], ['▁', 'salem', '▁', 'va', 'for', '▁the', '▁eighth', '▁straight', '▁world', 'history', '▁period', '▁sophomore', 's', '▁at', '▁', 'riverside', '▁high', '▁school', '▁watched', '▁the', '▁1959', '▁classic', '▁', 'ben', 'hur', '▁', 'tu', 'es', 'day', '▁the', '▁chariot', '▁races', '▁were', '▁pretty', '▁cool', '▁mi', 'cha', 'el', '▁bow', 'er', '▁said', '▁of', '▁the', '▁21', '1', 'minute', '▁film', '▁he', '▁and', '▁classmates', '▁have', '▁been', '▁watching', '▁in', '▁25', 'minute', '▁segments', '▁between', '▁roll', '▁call', '▁and', '▁free', 'reading', '▁and', '▁when', '▁', 'm', 'r', '▁frank', 's', '▁got', '▁back', '▁from', '▁the', '▁teachers', '▁lounge', '▁he', '▁told', '▁us', '▁', 'je', 's', 'us', '▁is', '▁in', '▁tomorrow', 's', '▁part', '▁bow', 'er', '▁said', '▁he', '▁dread', 's', '▁next', '▁week', '▁when', '▁the', '▁class', '▁will', '▁break', '▁into', '▁', 'ben', 'hur', '▁discussion', '▁groups', '▁and', '▁share', '▁their', '▁ancient', 'history', '▁unit', '▁journals', '▁[', 's', 'ep', ']', '▁[', 'cl', 's', ']']]\n"
          ]
        }
      ],
      "source": [
        "tokenizer = XLNetTokenizer.from_pretrained('xlnet-base-cased', do_lower_case=True)\n",
        "tokenized_texts = []\n",
        "for sent in list(sentences):\n",
        "  x = tokenizer.tokenize(sent)\n",
        "  tokenized_texts.append(x)\n",
        "print(tokenized_texts[:10])\n",
        "# tokenized_texts = [tokenizer.tokenize(sent) for sent in sentences]\n",
        "# print (\"Tokenize the first sentence:\")\n",
        "# print (tokenized_texts[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "87_kXUeT2-br"
      },
      "source": [
        "XLNet requires specifically formatted inputs. For each tokenized input sentence, we need to create:\n",
        "\n",
        "- **input ids**: a sequence of integers identifying each input token to its index number in the XLNet tokenizer vocabulary\n",
        "- **segment mask**: (optional) a sequence of 1s and 0s used to identify whether the input is one sentence or two sentences long. For one sentence inputs, this is simply a sequence of 0s. For two sentence inputs, there is a 0 for each token of the first sentence, followed by a 1 for each token of the second sentence\n",
        "- **attention mask**: (optional) a sequence of 1s and 0s, with 1s for all input tokens and 0s for all padding tokens (we'll detail this in the next paragraph)\n",
        "- **labels**: a single value of 1 or 0. In our task 1 means \"grammatical\" and 0 means \"ungrammatical\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xytsw1oIfnX0"
      },
      "source": [
        "Although we can have variable length input sentences, XLNet does requires our input arrays to be the same size. We address this by first choosing a maximum sentence length, and then padding and truncating our inputs until every input sequence is of the same length. \n",
        "\n",
        "To \"pad\" our inputs in this context means that if a sentence is shorter than the maximum sentence length, we simply add 0s to the end of the sequence until it is the maximum sentence length. \n",
        "\n",
        "If a sentence is longer than the maximum sentence length, then we simply truncate the end of the sequence, discarding anything that does not fit into our maximum sentence length.\n",
        "\n",
        "We pad and truncate our sequences so that they all become of length MAX_LEN (\"post\" indicates that we want to pad and truncate at the end of the sequence, as opposed to the beginning) `pad_sequences` is a utility function that we're borrowing from Keras. It simply handles the truncating and padding of Python lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cp9BPRd1tMIo"
      },
      "outputs": [],
      "source": [
        "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
        "MAX_LEN = 128"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mFhowDMohU4H"
      },
      "outputs": [],
      "source": [
        "# Use the XLNet tokenizer to convert the tokens to their index numbers in the XLNet vocabulary\n",
        "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tokenized_texts]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kDs-MYtYH8sL"
      },
      "outputs": [],
      "source": [
        "# Pad our input tokens\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhGulL1pExCT"
      },
      "source": [
        "Create the attention masks "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDoC24LeEv3N"
      },
      "outputs": [],
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  attention_masks.append(seq_mask)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aFbE-UHvsb7-"
      },
      "outputs": [],
      "source": [
        "# Use train_test_split to split our data into train and validation sets for training\n",
        "\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.2)\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
        "                                             random_state=2018, test_size=0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jw5K2A5Ko1RF"
      },
      "outputs": [],
      "source": [
        "# Convert all of our data into torch tensors, the required datatype for our model\n",
        "\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GEgLpFVlo1Z-"
      },
      "outputs": [],
      "source": [
        "# Select a batch size for training. For fine-tuning with XLNet, the authors recommend a batch size of 32, 48, or 128. We will use 32 here to avoid memory issues.\n",
        "batch_size = 32\n",
        "\n",
        "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
        "# with an iterator the entire dataset does not need to be loaded into memory\n",
        "\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNl8khAhPYju"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fwQ7JcuJQZ0o"
      },
      "source": [
        "Now that our input data is properly formatted, it's time to fine tune the XLNet model. \n",
        "\n",
        "For this task, we first want to modify the pre-trained model to give outputs for classification, and then we want to continue training the model on our dataset until that the entire model, end-to-end, is well-suited for our task. Thankfully, the huggingface pytorch implementation includes a set of interfaces designed for a variety of NLP tasks. Though these interfaces are all built on top of a trained model, each has different top layers and output types designed to accomodate their specific NLP task.  \n",
        "\n",
        "We'll load [XLNetForSequenceClassification](https://github.com/huggingface/pytorch-transformers/blob/master/pytorch_transformers/modeling_xlnet.py#L1076). This is the normal XLNet model with an added single linear layer on top for classification that we will use as a sentence classifier. As we feed input data, the entire pre-trained XLNet model and the additional untrained classification layer is trained on our specific task. \n",
        "\n",
        "### The Fine-Tuning Process\n",
        "\n",
        "Because the pre-trained model layers already encode a lot of information about the language, training the classifier is relatively inexpensive. Rather than training every layer in a large model from scratch, it's as if we have already trained the bottom layers 95% of where they need to be, and only really need to train the top layer, with a bit of tweaking going on in the lower levels to accomodate our task.\n",
        "\n",
        "Sometimes practicioners will opt to \"freeze\" certain layers when fine-tuning, or to apply different learning rates, apply diminishing learning rates, etc. all in an effort to preserve the good quality weights in the network and speed up training (often considerably). In fact, recent research on transfer learning models like BERT specifically has demonstrated that freezing the majority of the weights results in only minimal accuracy declines, but there are exceptions and broader rules of transfer learning that should also be considered. For example, if your task and fine-tuning dataset is very different from the dataset used to train the transfer learning model, freezing the weights may not be a good idea. We'll cover the broader scope of transfer learning in NLP in a future post.  \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WnQW9E-bBCRt"
      },
      "source": [
        "OK, let's load XLNet! There are a few different pre-trained XLNet models available. \"xlnet-base-cased\" means the version that has both upper and lowercase letters (\"cased\") and is the smaller version of the two (\"base\" vs \"large\")."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gFsCTp_mporB",
        "outputId": "a259970e-43d5-45a5-9363-c6065ff62a89"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at xlnet-base-cased were not used when initializing XLNetForSequenceClassification: ['lm_loss.bias', 'lm_loss.weight']\n",
            "- This IS expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing XLNetForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of XLNetForSequenceClassification were not initialized from the model checkpoint at xlnet-base-cased and are newly initialized: ['logits_proj.bias', 'logits_proj.weight', 'sequence_summary.summary.bias', 'sequence_summary.summary.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "XLNetForSequenceClassification(\n",
              "  (transformer): XLNetModel(\n",
              "    (word_embedding): Embedding(32000, 768)\n",
              "    (layer): ModuleList(\n",
              "      (0): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (1): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (2): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (3): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (4): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (5): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (6): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (7): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (8): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (9): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (10): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "      (11): XLNetLayer(\n",
              "        (rel_attn): XLNetRelativeAttention(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "        (ff): XLNetFeedForward(\n",
              "          (layer_norm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "          (layer_1): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          (layer_2): Linear(in_features=3072, out_features=768, bias=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "          (activation_function): GELUActivation()\n",
              "        )\n",
              "        (dropout): Dropout(p=0.1, inplace=False)\n",
              "      )\n",
              "    )\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (sequence_summary): SequenceSummary(\n",
              "    (summary): Linear(in_features=768, out_features=768, bias=True)\n",
              "    (activation): Tanh()\n",
              "    (first_dropout): Identity()\n",
              "    (last_dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (logits_proj): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 91,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load XLNEtForSequenceClassification, the pretrained XLNet model with a single linear classification layer on top. \n",
        "\n",
        "model = XLNetForSequenceClassification.from_pretrained(\"xlnet-base-cased\", num_labels=4)\n",
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8o-VEBobKwHk"
      },
      "source": [
        "Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\n",
        "\n",
        "For the purposes of fine-tuning, the authors recommend the following hyperparameters in the following ranges (broken down by which NLP dataset they are applied to):\n",
        "\n",
        "\n",
        "![alt text](https://i.imgur.com/AhirErN.png)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QxSMw0FrptiL"
      },
      "outputs": [],
      "source": [
        "\n",
        "param_optimizer = list(model.named_parameters())\n",
        "no_decay = ['bias', 'gamma', 'beta']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "     'weight_decay_rate': 0.0}\n",
        "]\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GLs72DuMODJO",
        "outputId": "e52ca6b2-08ba-4d86-c9c4-64efdd63f74f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# This variable contains all of the hyperparemeter information our training loop needs\n",
        "optimizer = AdamW(optimizer_grouped_parameters,\n",
        "                     lr=2e-5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_QXZhFb4LnV5"
      },
      "source": [
        "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a training phase and a validation phase. At each pass we need to:\n",
        "\n",
        "Training loop:\n",
        "- Tell the model to compute gradients by setting the model in train mode\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Clear out the gradients calculated in the previous pass. In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n",
        "- Forward pass (feed input data through the network)\n",
        "- Backward pass (backpropagation)\n",
        "- Tell the network to update parameters with optimizer.step()\n",
        "- Track variables for monitoring progress\n",
        "\n",
        "Evalution loop:\n",
        "- Tell the model not to compute gradients by setting th emodel in evaluation mode\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Forward pass (feed input data through the network)\n",
        "- Compute loss on our validation data and track variables for monitoring progress\n",
        "\n",
        "So please read carefully through the comments to get an understanding of what's happening. If you're unfamiliar with pytorch a quick look at some of their [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) will help show you that training loops really involve only a few simple steps; the rest is usually just decoration and logging.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9cQNvaZ9bnyy"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "6J-FYdx6nFE_",
        "outputId": "a0b9ba45-bf71-4a04-a21c-f1075829440a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:   0%|          | 0/3 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Train loss: 0.1534726702049527\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:  33%|███▎      | 1/3 [20:52<41:44, 1252.46s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9801971925133691\n",
            "Train loss: 0.038194453999394994\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rEpoch:  67%|██████▋   | 2/3 [41:47<20:53, 1253.85s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9805964052287581\n",
            "Train loss: 0.02158995175749119\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch: 100%|██████████| 3/3 [1:02:40<00:00, 1253.64s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.9747753267973857\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Store our loss and accuracy for plotting\n",
        "train_loss_set = []\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 3\n",
        "\n",
        "# trange is a tqdm wrapper around the normal python range\n",
        "for _ in trange(epochs, desc=\"Epoch\"):\n",
        "  \n",
        "  \n",
        "  # Training\n",
        "  \n",
        "  # Set our model to training mode (as opposed to evaluation mode)\n",
        "  model.train()\n",
        "  \n",
        "  # Tracking variables\n",
        "  tr_loss = 0\n",
        "  nb_tr_examples, nb_tr_steps = 0, 0\n",
        "  \n",
        "  # Train the data for one epoch\n",
        "  for step, batch in enumerate(train_dataloader):\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Clear out the gradients (by default they accumulate)\n",
        "    optimizer.zero_grad()\n",
        "    # Forward pass\n",
        "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
        "    loss = outputs[0]\n",
        "    logits = outputs[1]\n",
        "    train_loss_set.append(loss.item())    \n",
        "    # Backward pass\n",
        "    loss.backward()\n",
        "    # Update parameters and take a step using the computed gradient\n",
        "    optimizer.step()\n",
        "    \n",
        "    \n",
        "    # Update tracking variables\n",
        "    tr_loss += loss.item()\n",
        "    nb_tr_examples += b_input_ids.size(0)\n",
        "    nb_tr_steps += 1\n",
        "\n",
        "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
        "    \n",
        "    \n",
        "  # Validation\n",
        "\n",
        "  # Put model in evaluation mode to evaluate loss on the validation set\n",
        "  model.eval()\n",
        "\n",
        "  # Tracking variables \n",
        "  eval_loss, eval_accuracy = 0, 0\n",
        "  nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "  # Evaluate data for one epoch\n",
        "  for batch in validation_dataloader:\n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
        "    with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions\n",
        "      output = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
        "      logits = output[0]\n",
        "    \n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "    \n",
        "    eval_accuracy += tmp_eval_accuracy\n",
        "    nb_eval_steps += 1\n",
        "\n",
        "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "024d1c3eb53849638b4ac41b65095bb2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_49db97d095bd45feb29d24ff992efd1b",
            "placeholder": "​",
            "style": "IPY_MODEL_8a673928f54f4fb18fb4dc95d632a579",
            "value": " 798k/798k [00:01&lt;00:00, 718kB/s]"
          }
        },
        "1b0dd29367694dc7939f1a906ba58424": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_52d0e753d1094ac0bf4c510f23daf377",
              "IPY_MODEL_de75cff83a814914b5611bdfd2d04139",
              "IPY_MODEL_024d1c3eb53849638b4ac41b65095bb2"
            ],
            "layout": "IPY_MODEL_5d64b41c019949b48f12e637597cadba"
          }
        },
        "1f3f8243ff4c4a73b954d7f74b1e9d9e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "26810a377f0b464b8e4213bc7f916b03": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_64eb473908524150a5d117703edecc2d",
            "placeholder": "​",
            "style": "IPY_MODEL_d1413ac14e984be29c443813adabaa53",
            "value": "Downloading (…)lve/main/config.json: 100%"
          }
        },
        "3be949c1839b4f2a81db3c22721e9da7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "49db97d095bd45feb29d24ff992efd1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4c73f89368a24638813e2a91ac5a28ae": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f467ee5e7a4a410e9785777ebcbf578d",
            "max": 760,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_feabf21450ec4707b7ffe4661377e928",
            "value": 760
          }
        },
        "5248358598d14cd4a926681e7dad7204": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "52d0e753d1094ac0bf4c510f23daf377": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1f3f8243ff4c4a73b954d7f74b1e9d9e",
            "placeholder": "​",
            "style": "IPY_MODEL_d5d3619fede64862a0c0d5c0e3dee965",
            "value": "Downloading (…)ve/main/spiece.model: 100%"
          }
        },
        "5d64b41c019949b48f12e637597cadba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6148540e8507424bb3be6608447e95cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_26810a377f0b464b8e4213bc7f916b03",
              "IPY_MODEL_4c73f89368a24638813e2a91ac5a28ae",
              "IPY_MODEL_b45acac8abb2485fa9f84a84ff5d34c2"
            ],
            "layout": "IPY_MODEL_c9855f7ddba8429dbff90de71079e2d9"
          }
        },
        "64eb473908524150a5d117703edecc2d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a818b9950234ba5b5601d652287cb38": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8a673928f54f4fb18fb4dc95d632a579": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b45acac8abb2485fa9f84a84ff5d34c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5248358598d14cd4a926681e7dad7204",
            "placeholder": "​",
            "style": "IPY_MODEL_6a818b9950234ba5b5601d652287cb38",
            "value": " 760/760 [00:00&lt;00:00, 9.96kB/s]"
          }
        },
        "c9855f7ddba8429dbff90de71079e2d9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1413ac14e984be29c443813adabaa53": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d5d3619fede64862a0c0d5c0e3dee965": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "de75cff83a814914b5611bdfd2d04139": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ead0975fc9464153803e7037c40d4a5e",
            "max": 798011,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3be949c1839b4f2a81db3c22721e9da7",
            "value": 798011
          }
        },
        "ead0975fc9464153803e7037c40d4a5e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f467ee5e7a4a410e9785777ebcbf578d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "feabf21450ec4707b7ffe4661377e928": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}