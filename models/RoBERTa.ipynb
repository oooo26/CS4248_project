{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T6kLU2_V7wTs",
    "tags": []
   },
   "source": [
    "# RoBERTa\n",
    "\n",
    "Supported by [huggingface/transformers](https://github.com/huggingface/transformers), PyTorch version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "KT6gp8Hl-soo",
    "outputId": "6640810f-3cc2-4a39-c6ff-d848f2a9c93f"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "X-YH_tPd8bl4"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install numpy pandas scikit-learn\n",
    "!pip install transformers torchmetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TwE9zTIQAsl9",
    "outputId": "5df1fd9f-477c-45df-e3cf-42c9949a78f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch Device: cuda\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from torchmetrics import F1Score, Accuracy\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import (AutoModelForSequenceClassification,\n",
    "                          AutoTokenizer)\n",
    "from tqdm import tqdm\n",
    "\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"PyTorch Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7mboHZIkCd9E"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RoBERTa_L512_B16_LR3e-05_W.pth\n"
     ]
    }
   ],
   "source": [
    "# can be tuned\n",
    "MAX_LEN = 512\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 3e-5\n",
    "TRAIN_SIZE = 0.8\n",
    "\n",
    "# other constant\n",
    "# PROJ_DIR = \"drive/MyDrive/CS4248 Project/\"\n",
    "PROJ_DIR = \"./\"\n",
    "RANDOM_SEED = 123\n",
    "MODEL_NAME = \"RoBERTa_L\" + str(MAX_LEN) + \"_B\" + str(BATCH_SIZE) + \"_LR\" + str(LEARNING_RATE) + \"_W.pth\"\n",
    "print(MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GegxJRN1Ul3L"
   },
   "source": [
    "## Build Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "XyOT6ejKGEet"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, data, tokenizer, max_len):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.text = data['text']\n",
    "        self.label = data['label']\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        text_tok = self.tokenizer(\n",
    "            text=self.text.iloc[index],\n",
    "            max_length=self.max_len,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_token_type_ids=True\n",
    "        )\n",
    "        return {\n",
    "            'input_ids': torch.tensor(text_tok['input_ids'], dtype=torch.long).to(device),\n",
    "            'attention_mask': torch.tensor(text_tok['attention_mask'], dtype=torch.long).to(device),\n",
    "            'token_type_ids': torch.tensor(text_tok[\"token_type_ids\"], dtype=torch.long).to(device),\n",
    "            'labels': torch.tensor(self.label.iloc[index], dtype=torch.long).to(device)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "oQ3t01Dv7wTu",
    "outputId": "8b704fe5-9960-4af8-8877-e3f94f92c8d8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "all labels: [0 1 2 3]\n",
      "full data: (48854, 2)\n",
      "train data: (39083, 2)\n",
      "valid data: (9771, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10635</th>\n",
       "      <td>0</td>\n",
       "      <td>According to a study published Thursday by res...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32278</th>\n",
       "      <td>2</td>\n",
       "      <td>Gulf War Veteran Charlie McGrath Issues Import...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29715</th>\n",
       "      <td>2</td>\n",
       "      <td>Using Nature to Solve Natures ProblemsHeather ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>0</td>\n",
       "      <td>In a statement delivered to friends, family me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46173</th>\n",
       "      <td>3</td>\n",
       "      <td>But Zelaya's delegation immediately rejected t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7763</th>\n",
       "      <td>0</td>\n",
       "      <td>During a press conference Thursday at the Mass...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15377</th>\n",
       "      <td>1</td>\n",
       "      <td>School Does Something SICKENING To Kids On 9/1...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17730</th>\n",
       "      <td>1</td>\n",
       "      <td>He Saw This In The Distance, Did A Double Take...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28030</th>\n",
       "      <td>2</td>\n",
       "      <td>Jurors Still Angry About Hershberger Trial; Fo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15725</th>\n",
       "      <td>1</td>\n",
       "      <td>Islamic State Threatens Putin With Mass Terror...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>39083 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       label                                               text\n",
       "10635      0  According to a study published Thursday by res...\n",
       "32278      2  Gulf War Veteran Charlie McGrath Issues Import...\n",
       "29715      2  Using Nature to Solve Natures ProblemsHeather ...\n",
       "147        0  In a statement delivered to friends, family me...\n",
       "46173      3  But Zelaya's delegation immediately rejected t...\n",
       "...      ...                                                ...\n",
       "7763       0  During a press conference Thursday at the Mass...\n",
       "15377      1  School Does Something SICKENING To Kids On 9/1...\n",
       "17730      1  He Saw This In The Distance, Did A Double Take...\n",
       "28030      2  Jurors Still Angry About Hershberger Trial; Fo...\n",
       "15725      1  Islamic State Threatens Putin With Mass Terror...\n",
       "\n",
       "[39083 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fulltrain = pd.read_csv(PROJ_DIR + 'raw_data/fulltrain.csv', names = ['label', 'text'])\n",
    "fulltrain['label'] = fulltrain['label'] - 1     # label = 0, 1, 2, 3\n",
    "train, valid = train_test_split(\n",
    "    fulltrain, \n",
    "    train_size=TRAIN_SIZE, \n",
    "    shuffle=True,\n",
    "    random_state=RANDOM_SEED\n",
    ")\n",
    "\n",
    "print(f\"all labels: {fulltrain['label'].unique()}\")\n",
    "print(f\"full data: {fulltrain.shape}\")\n",
    "print(f\"train data: {train.shape}\")\n",
    "print(f\"valid data: {valid.shape}\")\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "_kLbCaLVB8RY"
   },
   "outputs": [],
   "source": [
    "# dataset and dataloader\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'roberta-base',\n",
    "    truncation=True,\n",
    "    do_lower_case=True\n",
    ")\n",
    "\n",
    "train_ds = MyDataset(train, tokenizer, MAX_LEN)\n",
    "valid_ds = MyDataset(valid, tokenizer, MAX_LEN)\n",
    "train_loader = DataLoader(\n",
    "    train_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "OxPzSErmRsts"
   },
   "outputs": [],
   "source": [
    "del fulltrain\n",
    "del train\n",
    "del valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KvANejP-Upma"
   },
   "source": [
    "## Fine-tune RoBERTa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i6jRbI4c7wTv",
    "outputId": "e0c0f551-7acf-4bb1-f91e-1ab6cd87fe59"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=768, out_features=4, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "hON1YrBvZqJn"
   },
   "outputs": [],
   "source": [
    "f1 = F1Score(task=\"multiclass\", num_classes=4, average=\"weighted\").to(device)\n",
    "acc = Accuracy(task=\"multiclass\", num_classes=4, average=\"weighted\").to(device)\n",
    "\n",
    "def train_process(dataloader, model, loss_fn, optimizer):\n",
    "    model.train()   # set train mode\n",
    "    size = len(dataloader.dataset)\n",
    "\n",
    "    for batch, data in tqdm(enumerate(dataloader, 0)):\n",
    "        pred_proba = model(**data).logits\n",
    "        loss = loss_fn(pred_proba, data['labels'])\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if batch % 500 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(data['labels'])\n",
    "            print(f\"\\tloss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def valid_process(dataloader, model, loss_fn):\n",
    "    model.eval()    # set eval mode\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    test_loss = 0\n",
    "    real_label = torch.Tensor([]).to(device)\n",
    "    pred_label = torch.Tensor([]).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch, data in tqdm(enumerate(dataloader, 0)):\n",
    "            pred_proba = model(**data).logits\n",
    "            real_label = torch.cat((real_label, data['labels']))\n",
    "            pred_label = torch.cat((pred_label, pred_proba.argmax(1)))\n",
    "            test_loss += loss_fn(pred_proba, data['labels']).item()\n",
    "    test_loss /= num_batches\n",
    "    f1_score = f1(pred_label, real_label)\n",
    "    accuracy = acc(pred_label, real_label)\n",
    "    print(f\"Test Process: \\n Avg loss: {test_loss:>8f}, Acc: {accuracy:>4f}, F1: {f1_score:>4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 537
    },
    "id": "2JNoydnoNPeT",
    "outputId": "b3d3e310-b1f7-41e3-ed26-81cbbecf3d7a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  2.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 1.360174  [   16/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [02:39,  3.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.010188  [ 8016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:16,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.003823  [16016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [07:54,  3.20it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.001379  [24016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [10:31,  3.18it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.121089  [32016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2443it [12:50,  3.17it/s]\n",
      "611it [01:20,  7.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Process: \n",
      " Avg loss: 0.052958, Acc: 0.982192, F1: 0.982196\n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.003878  [   16/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [02:37,  3.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.001184  [ 8016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:15,  3.16it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.000690  [16016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [07:53,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.000405  [24016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [10:31,  3.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.000617  [32016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2443it [12:52,  3.16it/s]\n",
      "611it [01:22,  7.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Process: \n",
      " Avg loss: 0.006513, Acc: 0.998158, F1: 0.998158\n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.000631  [   16/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "501it [02:38,  3.13it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.000349  [ 8016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1001it [05:16,  2.94it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.000772  [16016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1501it [07:54,  3.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.000406  [24016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2001it [10:32,  3.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tloss: 0.000363  [32016/39083]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2443it [12:52,  3.16it/s]\n",
      "611it [01:19,  7.64it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Process: \n",
      " Avg loss: 0.006487, Acc: 0.997851, F1: 0.997851\n",
      "\n",
      "Done!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.AdamW(params=model.parameters(), lr=LEARNING_RATE)\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print(f\"Epoch {epoch + 1}\\n-------------------------------\")\n",
    "    train_process(train_loader, model, loss_fn, optimizer)\n",
    "    valid_process(valid_loader, model, loss_fn)\n",
    "    scheduler.step()\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "KcYSTN9cazrY"
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), PROJ_DIR+\"model/\"+MODEL_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HW2DxbvrUiXv"
   },
   "source": [
    "## Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "01bXjdegbwoK",
    "outputId": "3a403dfc-3397-4b11-cf7c-62bc49d44516"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.dense.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'roberta.pooler.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.dense.weight', 'classifier.out_proj.weight', 'classifier.dense.bias', 'classifier.out_proj.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('roberta-base')\n",
    "model = AutoModelForSequenceClassification.from_pretrained('roberta-base', num_labels=4)\n",
    "model.load_state_dict(torch.load(PROJ_DIR+\"model/\"+MODEL_NAME))\n",
    "model.to(device)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "3vsPH56LUuDO"
   },
   "outputs": [],
   "source": [
    "balancedtest = pd.read_csv(PROJ_DIR + 'raw_data/balancedtest.csv', names = ['label', 'text'])\n",
    "balancedtest['label'] = balancedtest['label'] - 1\n",
    "\n",
    "test_ds = MyDataset(balancedtest, tokenizer, MAX_LEN)\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    num_workers=0\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvtUOJCMcgwK",
    "outputId": "c1092a58-2d8b-425b-fb85-c36d3f2b75d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2443it [05:18,  7.68it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Process: \n",
      " Avg loss: 0.001864, Acc: 0.999514, F1: 0.999514\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_process(train_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvtUOJCMcgwK",
    "outputId": "c1092a58-2d8b-425b-fb85-c36d3f2b75d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "611it [01:20,  7.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Process: \n",
      " Avg loss: 0.006487, Acc: 0.997851, F1: 0.997851\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_process(valid_loader, model, loss_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cvtUOJCMcgwK",
    "outputId": "c1092a58-2d8b-425b-fb85-c36d3f2b75d1"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "188it [00:23,  7.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Process: \n",
      " Avg loss: 2.173389, Acc: 0.698333, F1: 0.642204\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "valid_process(test_loader, model, loss_fn)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f394aca7ca06fed1e6064aef884364492d7cdda3614a461e02e6407fc40ba69"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
