{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "printable-split",
   "metadata": {},
   "source": [
    "\n",
    "# CS4248 Project - Truth of Varying Shades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "greater-synthesis",
   "metadata": {
    "id": "hmA6EzkQJ5jt",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import unidecode\n",
    "import string\n",
    "import csv\n",
    "import random\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.set_option('display.width', 100)\n",
    "pd.set_option('max_colwidth', None)\n",
    "from scipy.stats import linregress\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, confusion_matrix, classification_report, make_scorer\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow_addons.metrics import F1Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30277721-ba55-48d9-99d0-0e614ec68aab",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "31ad3a14-ae1a-4696-a114-5229171db5d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_graphs(history, metric):\n",
    "    plt.figure(figsize=(8, 6), dpi=100)\n",
    "    plt.plot(history.history[metric], 'r')\n",
    "    plt.plot(history.history['val_' + metric], 'b')\n",
    "    plt.title(f'Training and validation {metric}')\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(metric)\n",
    "    plt.legend([metric, 'val_'+metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "nutritional-yield",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "occasional-penalty",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "EMBEDDING_DIM = 100  # Dimension of the dense embedding. Defaults to 100\n",
    "MAXLEN = 1000  # Maximum length of all sequences. Defaults to 1000.\n",
    "\n",
    "# training\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "applied-rubber",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data pipeline\n",
    "\n",
    "The dataset is provided in a csv file. Each row of this file contains the following values separated by commas:\n",
    "- label: the class label of the text\n",
    "- text: the text content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e26814a8-04e4-4260-9db9-273619d8c810",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    # remove punctuation\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '' , text)\n",
    "    # remove accented characters from text, e.g. cafÃ©\n",
    "    text = unidecode.unidecode(text)\n",
    "    # some other preprocessing steps\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "420b8024-1f89-43e1-be8b-55573d63e7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset(file_path):\n",
    "    # load data\n",
    "    fulltrain = pd.read_csv(FULLTRAIN_CSV, names=['label', 'text'])\n",
    "    sentences, labels = fulltrain.text, fulltrain.label - 1  # -1 to make labels 0-index\n",
    "    print(f\"Found {len(sentences)} examples.\\n\")\n",
    "    \n",
    "    sentences = sentences.apply(preprocess_text)\n",
    "    labels = pd.get_dummies(labels)\n",
    "    print(f\"Label of first example: {labels.iloc[0].tolist()}\")\n",
    "    print(f\"Text of first example:\\n'{sentences[0]}'\\n\")\n",
    "    \n",
    "    # split dataset into train and validation\n",
    "    train_sentences, val_sentences, train_labels, val_labels = train_test_split(\n",
    "        sentences, labels, test_size=0.2, shuffle=True, stratify=labels, random_state=42)\n",
    "    print('After train val split:')\n",
    "    print(f\"There are {len(train_sentences)} examples for training.\")\n",
    "    print(f\"There are {len(val_sentences)} examples for validation.\\n\")\n",
    "    \n",
    "    # data pipeline\n",
    "    train = tf.data.Dataset.from_tensor_slices((train_sentences, train_labels))\\\n",
    "        .cache()\\\n",
    "        .shuffle(10 * BATCH_SIZE)\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    val = tf.data.Dataset.from_tensor_slices((val_sentences, val_labels))\\\n",
    "        .cache()\\\n",
    "        .shuffle(10 * BATCH_SIZE)\\\n",
    "        .batch(BATCH_SIZE)\\\n",
    "        .prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "    return train, val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a18966-38e4-44cd-9207-849741dfb25b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 48854 examples.\n",
      "\n",
      "Label of first example: [1, 0, 0, 0]\n",
      "Text of first example:\n",
      "'A little less than a decade ago hockey fans were blessed with a slate of games every night but on Thursday sources confirmed that for the ninth consecutive year NHL players have been locked out with very slim hopes of an agreement in sight It seems like just yesterday Martin St Louis and his Lightning teammates were raising the Stanley Cup high school hockey coach and onetime ESPN analyst Barry Melrose said Obviously Im still hoping the two sides can come together and reach an agreement but Im starting to think nobody really misses hockey anymore Nope Nobody but old Barry Id still love to catch an Atlanta Thrashers game Observers have noted that when arena doors do reopen the NHL will face the perhaps greater challenge of convincing fans to return to hockey instead of watching more popular sports like football basketball baseball and SlamBall '\n",
      "\n",
      "After train val split:\n",
      "There are 39083 examples for training.\n",
      "There are 9771 examples for validation.\n",
      "\n",
      "Metal device set to: Apple M2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 17:11:15.394197: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-03-19 17:11:15.395146: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    }
   ],
   "source": [
    "FULLTRAIN_CSV = './raw_data/fulltrain.csv'\n",
    "train_dataset, val_dataset = prepare_dataset(FULLTRAIN_CSV)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wired-fifteen",
   "metadata": {},
   "source": [
    "## Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "694451ca-fc60-43c3-9153-4f1b32e7eb42",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-19 17:11:19.040456: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-03-19 17:11:19.101101: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary contains 279409 words.\n",
      "First 20 words in the vocabulary: ['', '[UNK]', 'the', 'to', 'of', 'and', 'a', 'in', 'that', 'is', 'for', 'on', 'it', 'as', 'with', 'are', 'be', 'this', 'was', 'have']\n",
      "Index of unknown token is 1.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = tf.keras.layers.TextVectorization(\n",
    "    max_tokens=None,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAXLEN\n",
    ")\n",
    "tokenizer.adapt(train_dataset.map(lambda text, label: text))\n",
    "\n",
    "vocab = tokenizer.get_vocabulary(include_special_tokens=True)\n",
    "VOCAB_SIZE = tokenizer.vocabulary_size()\n",
    "\n",
    "print(f\"Vocabulary contains {VOCAB_SIZE} words.\")\n",
    "print(f\"First 20 words in the vocabulary: {vocab[:20]}\")\n",
    "print(f\"Index of unknown token is {vocab.index('[UNK]')}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "corresponding-choir",
   "metadata": {},
   "source": [
    "## Prepare pre-defined Embeddings\n",
    "\n",
    "We here use the 6B & 100d version of [GloVe](https://nlp.stanford.edu/projects/glove/) from Stanford."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "continental-pittsburgh",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "GLOVE_FILE = './glove.6B.100d.txt'\n",
    "\n",
    "GLOVE_EMBEDDINGS = {} # Initialize an empty embeddings index dictionary\n",
    "\n",
    "# Read file and fill GLOVE_EMBEDDINGS with its contents\n",
    "with open(GLOVE_FILE) as f:\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        GLOVE_EMBEDDINGS[word] = coefs\n",
    "\n",
    "# Initialize an empty numpy array with the appropriate size\n",
    "EMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE, EMBEDDING_DIM)) \n",
    "\n",
    "for i, word in enumerate(vocab):  # iterate over each word in the vocabulary\n",
    "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        EMBEDDINGS_MATRIX[i] = embedding_vector\n",
    "\n",
    "# Since the original GLOVE doesn't have an embedding for [UNK], let's make it the mean of all embeddings.\n",
    "# According to Pennington: \"I've found that just taking an average of all or a subset of the word vectors produces a good unknown vector.\"\n",
    "EMBEDDINGS_MATRIX[vocab.index('[UNK]')] = EMBEDDINGS_MATRIX.mean(axis=0) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "missing-solution",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "representative-taylor",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "def create_model(embedding_dim, maxlen, embeddings_matrix=None, unit=None, finetune_embedding=False):\n",
    "    \"\"\"create a basic bidirectional rnn with specified rnn/lstm/gru unit and whether to finetune embeddings or not\n",
    "       params:\n",
    "           unit: str, one of 'rnn', 'lstm', 'gru', default rnn\n",
    "           fintune_embedding: bool, whether to finetune pretrained embedding, default False\n",
    "       return:\n",
    "           the text classifier model\n",
    "    \"\"\"\n",
    "    if unit == 'lstm':\n",
    "        cell = tf.keras.layers.LSTM(64)\n",
    "    elif unit == 'gru':\n",
    "        cell = tf.keras.layers.GRU(64)\n",
    "    else:\n",
    "        cell = tf.keras.layers.RNN(64)\n",
    "        \n",
    "    model = tf.keras.Sequential([\n",
    "        tokenizer,  # text vectorization layer\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=encoder.vocabulary_size(),\n",
    "            output_dim=embedding_dim,\n",
    "            weights=embeddings_matrix,\n",
    "            trainable=finetune_embedding,\n",
    "            #mask_zero=True  # to handle variable sequence lengths\n",
    "        ),\n",
    "        tf.keras.layers.Bidirectional(cell),\n",
    "        tf.keras.layers.Dense(64, activation='relu'),\n",
    "        tf.keras.layers.Dense(4)\n",
    "    ])\n",
    "    \n",
    "    model.compile(loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "                  optimizer=tf.keras.optimizers.Adam(),\n",
    "                  metrics=['accuracy', F1Score(num_classes=4, average='macro')]) \n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "explicit-tokyo",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# basic lstm, without finetuing pretrained embeddings\n",
    "lstm_base = create_model(EMBEDDING_DIM, MAXLEN, [EMBEDDINGS_MATRIX], 'lstm', False)\n",
    "lstm_base_checkpoint_path = \"./checkpoints/my_checkpoint\"\n",
    "print(lstm_base.summary())\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping('val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=lstm_base_checkpoint_path, \n",
    "                                                save_weights_only=True, \n",
    "                                                save_best_only=True,\n",
    "                                                verbose=1)\n",
    "# train \n",
    "try:\n",
    "    lstm_base.load_weights(lstm_base_checkpoint_path)\n",
    "except:\n",
    "    history = lstm_base.fit(train_dataset, \n",
    "                            epochs=100,\n",
    "                            validation_data=val_dataset,\n",
    "                            callbacks=[early_stopping, checkpoint])\n",
    "    \n",
    "# evaluate \n",
    "loss, accuracy, f1 = lstm_base.evaluate(val_dataset)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}, F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8419550b-59f9-4432-b9af-4f1f380e8451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fintuned lstm\n",
    "lstm_finetune = create_model(EMBEDDING_DIM, MAXLEN, [EMBEDDINGS_MATRIX], 'lstm', True)\n",
    "lstm_finetune_checkpoint_path = \"./checkpoints/lstm_finetune_checkpoint\"\n",
    "print(lstm_finetune.summary())\n",
    "\n",
    "early_stopping = tf.keras.callbacks.EarlyStopping('val_loss', patience=5, restore_best_weights=True)\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(filepath=lstm_finetune_checkpoint_path, \n",
    "                                                save_weights_only=True, \n",
    "                                                save_best_only=True,\n",
    "                                                verbose=1)\n",
    "# train\n",
    "try:\n",
    "    lstm_finetune.load_weights(lstm_finetune_checkpoint_path)\n",
    "except:\n",
    "    history = lstm_finetune.fit(train_dataset, \n",
    "                                epochs=100,\n",
    "                                validation_data=val_dataset,\n",
    "                                callbacks=[early_stopping, checkpoint])\n",
    "# evaluate\n",
    "loss, accuracy, f1 = lstm_finetune.evaluate(val_dataset)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}, F1: {f1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "628f5288-691c-42b2-be7b-4d693d5a4b83",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93bdeaeb-89ec-4ac2-93ac-0de687034657",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_graphs(history, 'accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attempted-needle",
   "metadata": {},
   "source": [
    "**Check if the slope of `val_loss` curve, should be <= 0.0005.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "constant-cursor",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "val_loss = history.history['val_loss']\n",
    "slope, *_ = linregress(range(len(val_loss)), val_loss)\n",
    "print(f\"The slope of validation loss curve is {slope:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "flexible-gravity",
   "metadata": {
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "with open('./history/lstm_finetune_history.pkl', 'wb') as f:\n",
    "    pickle.dump(history.history, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "069846cf-3545-4210-88c0-1bf9822e91bf",
   "metadata": {},
   "source": [
    "## BERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "34499995-81ba-473d-bca1-e2bd111fb765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-text==2.11.* (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-text==2.11.*\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q -U \"tensorflow-text==2.11.*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1a571514-8e75-441b-b775-8d0bd4cf557f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tf-models-official==2.11.0\n",
      "  Using cached tf_models_official-2.11.0-py2.py3-none-any.whl (2.3 MB)\n",
      "Collecting gin-config\n",
      "  Using cached gin_config-0.5.0-py3-none-any.whl (61 kB)\n",
      "Collecting py-cpuinfo>=3.3.0\n",
      "  Downloading py_cpuinfo-9.0.0-py3-none-any.whl (22 kB)\n",
      "Requirement already satisfied: tensorflow-hub>=0.6.0 in /Users/rz/opt/anaconda3/lib/python3.9/site-packages (from tf-models-official==2.11.0) (0.13.0)\n",
      "Collecting kaggle>=1.3.9\n",
      "  Using cached kaggle-1.5.13.tar.gz (63 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting oauth2client\n",
      "  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\n",
      "Collecting tensorflow-model-optimization>=0.4.1\n",
      "  Using cached tensorflow_model_optimization-0.7.3-py2.py3-none-any.whl (238 kB)\n",
      "Requirement already satisfied: Pillow in /Users/rz/opt/anaconda3/lib/python3.9/site-packages (from tf-models-official==2.11.0) (9.0.1)\n",
      "Requirement already satisfied: psutil>=5.4.3 in /Users/rz/opt/anaconda3/lib/python3.9/site-packages (from tf-models-official==2.11.0) (5.8.0)\n",
      "Collecting tensorflow-datasets\n",
      "  Using cached tensorflow_datasets-4.8.3-py3-none-any.whl (5.4 MB)\n",
      "Collecting pyyaml<6.0,>=5.1\n",
      "  Using cached PyYAML-5.4.1.tar.gz (175 kB)\n",
      "  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement tensorflow-text~=2.11.0 (from tf-models-official) (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for tensorflow-text~=2.11.0\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install tf-models-official==2.11.0  # to use AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "330e2265-eb3d-4229-a32b-6b8be17d9dd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow_hub in /Users/rz/opt/anaconda3/lib/python3.9/site-packages (0.13.0)\n",
      "Requirement already satisfied: numpy>=1.12.0 in /Users/rz/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_hub) (1.21.5)\n",
      "Requirement already satisfied: protobuf>=3.19.6 in /Users/rz/opt/anaconda3/lib/python3.9/site-packages (from tensorflow_hub) (4.22.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fa0fbd1c-9f0b-4945-832b-78ee741ebca9",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_text\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtext\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mofficial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnlp\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m optimization\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow_text as text\n",
    "from official.nlp import optimization  # to create AdamW optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a6be18e-0121-4e2c-b79c-ab6700374386",
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are many pretrained BERT models available from TF Hub\n",
    "## Small BERT: same general architecture but fewer Transformer blocks, tradeoff between speed and quality\n",
    "bert_model_name = 'small_bert/bert_en_uncased_L-4_H-512_A-8' \n",
    "\n",
    "map_name_to_handle = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_L-12_H-768_A-12/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_L-12_H-768_A-12/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_L-12_H-768_A-12/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-2_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-4_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-6_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-8_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-10_H-768_A-12/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-128_A-2/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-256_A-4/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-512_A-8/1',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/small_bert/bert_en_uncased_L-12_H-768_A-12/1',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_base/2',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/google/electra_small/2',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/google/electra_base/2',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/google/experts/bert/pubmed/2',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/google/experts/bert/wiki_books/2',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/talkheads_ggelu_bert_en_base/1',\n",
    "}\n",
    "\n",
    "map_model_to_preprocess = {\n",
    "    'bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_en_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_cased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-2_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-4_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-6_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-8_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-10_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-128_A-2':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-256_A-4':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-512_A-8':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'small_bert/bert_en_uncased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'bert_multi_cased_L-12_H-768_A-12':\n",
    "        'https://tfhub.dev/tensorflow/bert_multi_cased_preprocess/3',\n",
    "    'albert_en_base':\n",
    "        'https://tfhub.dev/tensorflow/albert_en_preprocess/3',\n",
    "    'electra_small':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'electra_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_pubmed':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'experts_wiki_books':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "    'talking-heads_base':\n",
    "        'https://tfhub.dev/tensorflow/bert_en_uncased_preprocess/3',\n",
    "}\n",
    "\n",
    "tfhub_handle_encoder = map_name_to_handle[bert_model_name]\n",
    "tfhub_handle_preprocess = map_model_to_preprocess[bert_model_name]\n",
    "\n",
    "print(f'BERT model selected           : {tfhub_handle_encoder}')\n",
    "print(f'Preprocess model auto-selected: {tfhub_handle_preprocess}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178b4442-4ab8-494c-bc03-8f49f6267866",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_classifier_model():\n",
    "    # create layers\n",
    "    preprocessing_layer = hub.KerasLayer(tfhub_handle_preprocess, name='preprocessing')\n",
    "    encoder = hub.KerasLayer(tfhub_handle_encoder, trainable=True, name='BERT_encoder')\n",
    "\n",
    "    # use Model API to build a model\n",
    "    text_input = tf.keras.layers.Input(shape=(), dtype=tf.string, name='text') # specify input\n",
    "    encoder_inputs = preprocessing_layer(text_input)  # pass through preprocessing layer\n",
    "    outputs = encoder(encoder_inputs)  # pass through BERT encoder\n",
    "    net = outputs['pooled_output']  # (B, H), get the embedding of the entire news\n",
    "    net = tf.keras.layers.Dropout(0.1)(net)  # prevent overfitting\n",
    "    net = tf.keras.layers.Dense(4, name='classifier')(net)\n",
    "    return tf.keras.Model(text_input, net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1c43641-85ac-4dca-ac8f-7e56ca3da7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model = build_classifier_model()\n",
    "tf.keras.utils.plot_model(bert_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6e8391-6ea7-4d21-ac43-76e7629c7ddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 5\n",
    "steps_per_epoch = tf.data.cardinality(train_dataset).numpy()\n",
    "num_train_steps = steps_per_epoch * epochs\n",
    "num_warmup_steps = int(0.1*num_train_steps)\n",
    "\n",
    "init_lr = 3e-5\n",
    "optimizer = optimization.create_optimizer(init_lr=init_lr,\n",
    "                                          num_train_steps=num_train_steps,\n",
    "                                          num_warmup_steps=num_warmup_steps,\n",
    "                                          optimizer_type='adamw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ba6654-73f5-4e25-9a58-33f5789486c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_model.compile(\n",
    "    loss=tf.keras.losses.CategoricalCrossentropy(from_logits=True),\n",
    "    metrics=['accuracy', F1Score(num_classes=4, average='macro')]\n",
    "    optimizer=optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13855ab8-5c21-4115-ad46-6772e7a5c6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Training model with {tfhub_handle_encoder}')\n",
    "history = bert_model.fit(x=train_ds,\n",
    "                               validation_data=val_ds,\n",
    "                               epochs=epochs,\n",
    "                               callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9378d975-daf6-46a8-87f9-134ac0284fa4",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, accuracy = bert_model.evaluate(test_ds)\n",
    "print(f'Loss: {ltrain_datasett(f'Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
